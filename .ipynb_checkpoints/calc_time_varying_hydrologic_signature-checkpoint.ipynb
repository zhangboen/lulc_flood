{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450756c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "#   This script is not finished !!!!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import os,glob\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb79e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQ(df):\n",
    "    # eliminate invalid records\n",
    "    df1 = df.loc[df.Q.apply(lambda x: not isinstance(x, str)),:]\n",
    "    df2 = df.loc[df.Q.apply(lambda x: isinstance(x, str)),:]\n",
    "    try:\n",
    "        df2 = df2.loc[df2.Q.str.match('\\d+'),:]\n",
    "    except:\n",
    "        pass\n",
    "    df = pd.concat([df1, df2])\n",
    "    df['Q'] = df.Q.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def del_unreliableQ(df):\n",
    "    '''\n",
    "    all records are rounded to three decimal places\n",
    "    observations less than 0 were flagged as suspected\n",
    "    observations with more than ten consecutive equal values greater than 0 were flagged as suspected\n",
    "    '''\n",
    "    df = df.loc[df.Q>=0,:].reset_index()\n",
    "    df['Q'] = df['Q'].round(3)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').set_index('date')\n",
    "    index = pd.date_range(df.index[0], df.index[-1], freq = 'D')\n",
    "    df = df.reindex(index)\n",
    "    df1 = df.diff()\n",
    "    df1 = df1.where(df1==0, 1).diff()\n",
    "    start = np.where(df1.values==-1)[0]\n",
    "    end = np.where(df1.values==1)[0]\n",
    "    if len(start) == 0 or len(end) == 0:\n",
    "        # must no less than zero\n",
    "        df = df.loc[df.Q>=0,:]\n",
    "        return (df)\n",
    "    if start[0] > end[0]:\n",
    "        start = np.array([0]+start.tolist())\n",
    "    if start[-1] > end[-1]:\n",
    "        end = np.array(end.tolist()+[df1.shape[0]+10])\n",
    "    duration = end - start\n",
    "    start = start[duration>=10]\n",
    "    end = end[duration>=10]\n",
    "    del_idx = np.array([item for a,b in zip(start,end) for item in np.arange(a+1,b+2).tolist()])\n",
    "    del_idx = del_idx[del_idx<df.shape[0]]\n",
    "    if len(del_idx) > 0:\n",
    "        df.drop(df.index[del_idx], inplace = True)\n",
    "    # must no less than zero\n",
    "    df = df.loc[df.Q>=0,:]\n",
    "    return (df)\n",
    "\n",
    "def del_outlierQ(df):\n",
    "    '''\n",
    "        Based on a previously suggested approach for evaluating temperature series (Klein Tank et al., 2009), \n",
    "        daily streamflow values are declared as outliers if values of log (Q+0.01) are larger or smaller than \n",
    "        the mean value of log (Q+0.01) plus or minus 6 times the standard deviation of log (Q+0.01) computed for \n",
    "        that calendar day for the entire length of the series. The mean and standard deviation are computed for \n",
    "        a 5-day window centred on the calendar day to ensure that a sufficient amount of data is considered. \n",
    "        The log-transformation is used to account for the skewness of the distribution of daily streamflow values \n",
    "        and 0.01 was added because the logarithm of zero is undefined. Outliers are flagged as suspect. \n",
    "        The rationale underlying this rule is that unusually large or small values are often associated with observational issues. \n",
    "        The 6 standard-deviation threshold is a compromise, aiming at screening out outliers that could come from \n",
    "        instrument malfunction, while not flagging extreme floods or low flows.\n",
    "    '''\n",
    "    df['logQ'] = np.log(df['Q']+0.01)\n",
    "    df['doy'] = df.index.dayofyear\n",
    "    df['year'] = df.index.year\n",
    "    df = df.pivot_table(index = 'doy', columns = 'year', values = 'logQ').reset_index()\n",
    "    def tmp(x0):\n",
    "        x = np.arange(x0-2, x0+3) \n",
    "        x = np.where(x <= 0, x + 366, x)\n",
    "        x = np.where(x > 366, x - 366, x)\n",
    "        s = df.loc[df.doy.isin(x),:].drop(columns=['doy']).values.flatten()\n",
    "        ave = np.nanmean(s)\n",
    "        std = np.nanstd(s)\n",
    "        low = ave - std * 6\n",
    "        upp = ave + std * 6\n",
    "        return (x0, low, upp)\n",
    "    thres = list(map(tmp, np.arange(1, 367)))\n",
    "    thres = pd.DataFrame(data = np.array(thres), columns = ['doy','low','upp'])\n",
    "    df = df.merge(thres, on = 'doy').set_index('doy')\n",
    "    df.iloc[:,:(df.shape[1]-2)] = df.iloc[:,:(df.shape[1]-2)].where(df.iloc[:,:(df.shape[1]-2)].lt(df['upp'], axis=0))\n",
    "    df.iloc[:,:(df.shape[1]-2)] = df.iloc[:,:(df.shape[1]-2)].where(df.iloc[:,:(df.shape[1]-2)].gt(df['low'], axis=0))\n",
    "    df = df.drop(columns = ['low','upp']).stack().reset_index(name='logQ')\n",
    "    df['Q'] = np.exp(df['logQ']) - 0.01\n",
    "    df['Q'] = np.where(df['Q'].abs()<1e-6, 0, df['Q'])\n",
    "    df['date'] = pd.to_datetime(df['level_1'].astype(str) + '-' + df['doy'].astype(str), format='%Y-%j')\n",
    "    df = df[['date','Q']].sort_values('date').set_index('date')\n",
    "    return df\n",
    "\n",
    "def Eckhardt(Q, alpha=.98, BFI=.80, re=1):\n",
    "    \"\"\"\n",
    "    Recursive digital filter for baseflow separation. Based on Eckhardt, 2004.\\n\n",
    "    Q : array of discharge measurements\\n\n",
    "    alpha : filter parameter\\n\n",
    "    BFI : BFI_max (maximum baseflow index)\\n\n",
    "    re : number of times to run filter\n",
    "    \"\"\"\n",
    "    Q = np.array(Q)\n",
    "    f = np.zeros(len(Q))\n",
    "    f[0] = Q[0]\n",
    "    for t in np.arange(1,len(Q)):\n",
    "        # algorithm\n",
    "        f[t] = ((1 - BFI) * alpha * f[t-1] + (1 - alpha) * BFI * Q[t]) / (1 - alpha * BFI)\n",
    "        if f[t] > Q[t]:\n",
    "            f[t] = Q[t]\n",
    "    # calls method again if multiple passes are specified\n",
    "    return np.nansum(f)/np.nansum(Q)\n",
    "\n",
    "# Function to identify and delete the smaller sample within 5 days\n",
    "def delete_smaller_sample(df):\n",
    "    while True:\n",
    "        intervals = (df.index[1:] - df.index[:-1]).days\n",
    "        if (intervals > 5).all():\n",
    "            break\n",
    "        indices1 = df.index[:-1][intervals<=5]\n",
    "        indices2 = df.index[1:][intervals<=5]\n",
    "        for int1,int2 in zip(indices1,indices2):\n",
    "            if int1 not in df.index:\n",
    "                continue\n",
    "            if df.loc[int1] < df.loc[int2]:\n",
    "                df.drop(int1, inplace=True)\n",
    "            else:\n",
    "                df.drop(int2, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate lag time between rainfall peak and discharge peak\n",
    "def calc_lagT(pr_dis, p = 95):\n",
    "    tmp = pr_dis.unstack(level=-1)\n",
    "    # use scipy to find local discharge peaks\n",
    "    R95p = tmp.loc[tmp.pr>=0.1,'pr'].quantile(p/100)\n",
    "    peaks_pr, _ = find_peaks(tmp.pr.values, height=R95p) # find R95p rainfall peaks\n",
    "    peaks_dis, _ = find_peaks(tmp.dis.values, height=0)\n",
    "    # remove peaks within five days\n",
    "    df_peaks_pr = tmp.iloc[peaks_pr,:].pr\n",
    "    df_peaks_pr = delete_smaller_sample(df_peaks_pr)\n",
    "    df_peaks_dis = tmp.iloc[peaks_dis,:].dis\n",
    "    df_peaks_dis = delete_smaller_sample(df_peaks_dis)\n",
    "    # calculate lag time bewteen rainfall peaks and discharge peaks\n",
    "    lagT = []\n",
    "    noResponse = 0\n",
    "    for index_pr in df_peaks_pr.index:\n",
    "        index_dis = df_peaks_dis.loc[df_peaks_dis.index>=index_pr]\n",
    "        if index_dis.shape[0] == 0:\n",
    "            noResponse += 1\n",
    "            continue\n",
    "        index_dis = index_dis.iloc[[0]].index[0]\n",
    "        if df_peaks_pr.loc[(df_peaks_pr.index>index_pr)&(df_peaks_pr.index<=index_dis)].shape[0] > 0:\n",
    "            noResponse += 1\n",
    "            continue\n",
    "        lagT.append((index_dis - index_pr).days)\n",
    "    if df_peaks_pr.shape[0] == 0:\n",
    "        noResRatio = 1\n",
    "    else:\n",
    "        noResRatio = noResponse/df_peaks_pr.shape[0]\n",
    "    return (pd.Series([np.mean(lagT), noResRatio], index = ['lagT','noResRatio',]))\n",
    "\n",
    "def calc_hs(df):\n",
    "    '''df should be a dataframe, include four columns: date, pr, dis, lat, and darea'''\n",
    "    if len(set(['date','pr','dis','lat','darea'])-set(df.columns.tolist())) != 0:\n",
    "        raise Exception ('the input df misses columns: date, pr, dis, and darea') \n",
    "\n",
    "    # transform streamflow to specific discharge\n",
    "    df['dis'] = df.dis.values / df.darea.values\n",
    "    lat = df.lat.values[0]\n",
    "    df = df.drop(columns=['darea','lat']).set_index('date')\n",
    "\n",
    "    newtime = pd.date_range(df.index.values[0], df.index.values[-1], freq = 'D')\n",
    "    df = df.reindex(newtime)\n",
    "\n",
    "    # discharge quantile\n",
    "    Q = df.loc[df.dis>0,'dis'].quantile([.05, .1, .5, .95])\n",
    "\n",
    "    # event duration\n",
    "    tmp1 = (df[['dis']] > Q.loc[0.5] * 9) * 1\n",
    "    tmp2 = (df[['dis']] < Q.loc[0.5] * 0.2) * 1\n",
    "    def func(x):\n",
    "        y = (np.diff(x) != 0).astype('int').cumsum()\n",
    "        y = np.hstack([np.nan, y])\n",
    "        y = pd.DataFrame({'x':x,'y':y})\n",
    "        y = y.loc[y.x==1,:].groupby('y').size().mean()\n",
    "        return np.array([y])\n",
    "    high_q_dur = tmp1.agg(func); high_q_dur = high_q_dur.fillna(0).squeeze()\n",
    "    low_q_dur = tmp2.agg(func); low_q_dur = low_q_dur.fillna(0).squeeze()\n",
    "\n",
    "    # calculate some hydrologic signatures\n",
    "    HS = pd.Series([\n",
    "        # Mean daily runoff\n",
    "        df['dis'].mean(), \n",
    "        # runoff ratio\n",
    "        df['dis'].mean() / df['pr'].mean(),  \n",
    "        # slope of the flow duration curve \n",
    "        (np.log(df.loc[df.dis>0,'dis'].quantile(.33)) - np.log(df.loc[df.dis>0,'dis'].quantile(.66))) / (0.66-0.33), \n",
    "        # runoff Q5 and Q95\n",
    "        Q.loc[0.05],\n",
    "        Q.loc[0.95],\n",
    "        # ratio of Q10 to Q50 to indicate groundwater\n",
    "        Q.loc[0.1] / Q.loc[0.5],\n",
    "        # frequency of high flows, low flows, and zero flows\n",
    "        (df['dis']>Q.loc[0.5]*9).sum(),\n",
    "        (df['dis']<Q.loc[0.5]*.2).sum(),\n",
    "        (df['dis']==0).sum(),\n",
    "        # variability coefficient\n",
    "        df['dis'].std() / df['dis'].mean(),\n",
    "        # event duration\n",
    "        high_q_dur, low_q_dur,\n",
    "        # BFI\n",
    "        df.dis.agg(Eckhardt),\n",
    "    ], index = [\n",
    "        'q_mean', 'runoff_ratio', 'slope_fdc', 'Q5', 'Q95', 'Q10_50', 'high_q_freq', 'low_q_freq', 'zero_q_freq', 'cv', 'high_q_dur', 'low_q_dur', 'BFI',\n",
    "    ])\n",
    "\n",
    "    # calculate lag time between peak rainfall and peak discharge \n",
    "    df_combine = df.reset_index().rename(columns={'index':'date'}).melt(id_vars = 'date')\n",
    "    df_combine = df_combine.rename(columns={'variable':'name'})\n",
    "    df_combine = df_combine.set_index(['date','name'])\n",
    "    df_lagT = df_combine.agg(calc_lagT).T.reset_index()\n",
    "    HS = pd.concat([HS, df_lagT[['lagT','noResRatio']].squeeze()])\n",
    "\n",
    "    # transform dataframe to hydrological-year cycle and then calculate some HS\n",
    "    # generally, 1 October to 30 September in the Northern Hemisphere, 1 July to 30 June in the Southern Hemisphere (https://glossary.ametsoc.org/wiki/Water_year)\n",
    "    \n",
    "    if lat >= 0:\n",
    "        start,end = 10,9\n",
    "    else:\n",
    "        start,end = 7,6\n",
    "    year1 = df.index.year.values[0]\n",
    "    year2 = df.index.year.values[-1]\n",
    "    hy1 = pd.to_datetime('%d-%02d-01'%(year1,start))\n",
    "    hy2 = pd.to_datetime('%d-%02d-30'%(year2,end))\n",
    "    \n",
    "    # transform time to hydrologic-cycle time\n",
    "    df = df.loc[(df.index>=hy1)&(df.index<=hy2),:]\n",
    "    df.index = pd.date_range('%d-1-1'%year1, periods = df.shape[0], freq = 'D')\n",
    "\n",
    "    # calculate mean annual rainfall and flashiness index\n",
    "    p_mean0 = df.groupby(df.index.year)['pr'].sum().mean()\n",
    "    def funcs(x):\n",
    "        a = np.abs(x.diff()).sum()\n",
    "        b = x.sum()\n",
    "        if b == 0 or np.isinf(b):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return a/b\n",
    "    FI0 = df.groupby(df.index.year)['dis'].apply(funcs).mean()\n",
    "    HS = pd.concat([HS, pd.Series([FI0,p_mean0], index = ['FI','p_mean'])])\n",
    "\n",
    "    # calculate other hydrologic signatures\n",
    "    # 1. stream_elas\n",
    "    mq_tot = df.dis.mean()\n",
    "    mp_tot = df.pr.mean()\n",
    "    mq = df.groupby(df.index.year)['dis'].mean()\n",
    "    mp = df.groupby(df.index.year)['pr'].mean()\n",
    "    dp = mp - mp_tot\n",
    "    dq = mq - mq_tot\n",
    "    stream_elas0 = ((dq/mq_tot)/(dp/mp_tot)).median()\n",
    "    HS = pd.concat([HS, pd.Series([stream_elas0], index = ['stream_elas'])])\n",
    "\n",
    "    # 2. hfd_mean\n",
    "    hfd_mean0 = df.groupby(df.index.year)['dis'].apply(lambda x: np.abs(x.cumsum() - x.sum()*0.5).argmin())\n",
    "    hfd_mean0 = hfd_mean0.where(hfd_mean0<365)\n",
    "    hfd_mean0 = hfd_mean0.mean(skipna=True)\n",
    "    HS = pd.concat([HS, pd.Series([hfd_mean0], index = ['hfd_mean'])])\n",
    "\n",
    "    return (HS)\n",
    "\n",
    "# read rainfall\n",
    "def readPr(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df.set_index('ohdb_id')\n",
    "    df = df.loc[:,df.columns.str.match('\\d+_P$')].T.reset_index()\n",
    "    df['date'] = pd.to_datetime(df['index'].str[:8], format = '%Y%m%d')\n",
    "    df = df.drop(columns=['index'])\n",
    "    print(fname)\n",
    "    return (df)\n",
    "fnames = glob.glob('../data_mswx/*daily_meteo*csv')\n",
    "pool = mp.Pool(8)\n",
    "df_pr = pool.map(readPr, fnames)\n",
    "df_pr = pd.concat(df_pr)\n",
    "print(df_pr.shape)\n",
    "\n",
    "df_attr = pd.read_csv('../data/basin_attributes.csv')\n",
    "\n",
    "def main(ohdb_id):\n",
    "    df_pr0 = df_pr[['date',ohdb_id]].rename(columns={ohdb_id:'pr'})\n",
    "    df_dis = pd.read_csv(f'../../data/OHDB/OHDB_v0.2.3/OHDB_data/discharge/daily/{ohdb_id}.csv')\n",
    "    df_dis['date'] = pd.to_datetime(df_dis['date'])\n",
    "    # read\n",
    "    df_dis = cleanQ(df_dis)\n",
    "    # quality check\n",
    "    df_dis = del_unreliableQ(df_dis)\n",
    "    # delete outliers\n",
    "    df_dis = del_outlierQ(df_dis).reset_index().rename(columns={'index':'date'})\n",
    "    df = df_pr0.merge(df_dis, on = 'date')\n",
    "    df = df.sort_values('date',ascending=True).rename(columns={'Q':'dis'})\n",
    "    df['darea'] = df_attr.loc[df_attr.ohdb_id==ohdb_id,'gritDarea'].values[0]\n",
    "    df['lat'] = df_attr.loc[df_attr.ohdb_id==ohdb_id,'ohdb_latitude'].values[0]\n",
    "    \n",
    "    df_event = pd.read_csv('../data/dis_OHDB_seasonal4_Qmin7_Qmax7_1982-2023.csv')\n",
    "    df_event0 = df_event.loc[df_event.ohdb_id==ohdb_id,['Qmax7date','Qmin7date','']]\n",
    "    \n",
    "    HS = calc_hs(df)\n",
    "    HS.name = ohdb_id\n",
    "    print(ohdb_id)\n",
    "    return (HS)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(12)\n",
    "    HS = pool.map(main, df_attr.ohdb_id.values)\n",
    "    HS = pd.concat(HS, axis = 1)\n",
    "    HS = HS.T.reset_index()\n",
    "    HS.to_csv('../data/time_varying_hydrologic_signatures.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9788b33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>countDay</th>\n",
       "      <th>Qmax7</th>\n",
       "      <th>Qmin7</th>\n",
       "      <th>Qmax7date</th>\n",
       "      <th>Qmin7date</th>\n",
       "      <th>ohdb_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DJF</td>\n",
       "      <td>1995</td>\n",
       "      <td>84</td>\n",
       "      <td>22.264286</td>\n",
       "      <td>4.037143</td>\n",
       "      <td>1995-12-17</td>\n",
       "      <td>1995-02-20</td>\n",
       "      <td>OHDB_009000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DJF</td>\n",
       "      <td>1996</td>\n",
       "      <td>91</td>\n",
       "      <td>34.954285</td>\n",
       "      <td>11.611429</td>\n",
       "      <td>1996-02-10</td>\n",
       "      <td>1996-12-29</td>\n",
       "      <td>OHDB_009000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DJF</td>\n",
       "      <td>1997</td>\n",
       "      <td>90</td>\n",
       "      <td>41.197143</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>1997-01-26</td>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>OHDB_009000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DJF</td>\n",
       "      <td>1998</td>\n",
       "      <td>90</td>\n",
       "      <td>25.871428</td>\n",
       "      <td>1.758571</td>\n",
       "      <td>1998-12-07</td>\n",
       "      <td>1998-02-07</td>\n",
       "      <td>OHDB_009000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DJF</td>\n",
       "      <td>1999</td>\n",
       "      <td>90</td>\n",
       "      <td>43.407144</td>\n",
       "      <td>13.960000</td>\n",
       "      <td>1999-12-27</td>\n",
       "      <td>1999-12-05</td>\n",
       "      <td>OHDB_009000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512589</th>\n",
       "      <td>SON</td>\n",
       "      <td>2018</td>\n",
       "      <td>91</td>\n",
       "      <td>0.837286</td>\n",
       "      <td>0.195714</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>OHDB_004000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512590</th>\n",
       "      <td>SON</td>\n",
       "      <td>2019</td>\n",
       "      <td>91</td>\n",
       "      <td>0.495429</td>\n",
       "      <td>0.037286</td>\n",
       "      <td>2019-11-04</td>\n",
       "      <td>2019-09-26</td>\n",
       "      <td>OHDB_004000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512591</th>\n",
       "      <td>SON</td>\n",
       "      <td>2020</td>\n",
       "      <td>91</td>\n",
       "      <td>0.798571</td>\n",
       "      <td>0.107571</td>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>OHDB_004000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512592</th>\n",
       "      <td>SON</td>\n",
       "      <td>2021</td>\n",
       "      <td>91</td>\n",
       "      <td>11.773143</td>\n",
       "      <td>0.209571</td>\n",
       "      <td>2021-09-05</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>OHDB_004000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512593</th>\n",
       "      <td>SON</td>\n",
       "      <td>2022</td>\n",
       "      <td>91</td>\n",
       "      <td>9.386714</td>\n",
       "      <td>0.179143</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>2022-09-19</td>\n",
       "      <td>OHDB_004000197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1512594 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        season  year  countDay  ...   Qmax7date   Qmin7date         ohdb_id\n",
       "0          DJF  1995        84  ...  1995-12-17  1995-02-20  OHDB_009000845\n",
       "1          DJF  1996        91  ...  1996-02-10  1996-12-29  OHDB_009000845\n",
       "2          DJF  1997        90  ...  1997-01-26  1997-12-31  OHDB_009000845\n",
       "3          DJF  1998        90  ...  1998-12-07  1998-02-07  OHDB_009000845\n",
       "4          DJF  1999        90  ...  1999-12-27  1999-12-05  OHDB_009000845\n",
       "...        ...   ...       ...  ...         ...         ...             ...\n",
       "1512589    SON  2018        91  ...  2018-11-28  2018-09-30  OHDB_004000197\n",
       "1512590    SON  2019        91  ...  2019-11-04  2019-09-26  OHDB_004000197\n",
       "1512591    SON  2020        91  ...  2020-10-30  2020-10-06  OHDB_004000197\n",
       "1512592    SON  2021        91  ...  2021-09-05  2021-10-31  OHDB_004000197\n",
       "1512593    SON  2022        91  ...  2022-11-30  2022-09-19  OHDB_004000197\n",
       "\n",
       "[1512594 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_event = pd.read_csv('../data/dis_OHDB_seasonal4_Qmin7_Qmax7_1982-2023.csv')\n",
    "df_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
