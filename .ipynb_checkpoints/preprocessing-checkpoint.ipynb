{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5c753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyogrio import read_dataframe,write_dataframe\n",
    "import geopandas as gpd\n",
    "import os,glob,sys,time,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from parallel_pandas import ParallelPandas\n",
    "from tqdm.auto import tqdm\n",
    "ParallelPandas.initialize(n_cpu=24, split_factor=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc330ab3",
   "metadata": {},
   "source": [
    "### get basin boundary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bff36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = glob.glob('../../data/GRIT/full_catchment/GRIT_full_catchment_*_EPSG8857_simplify_final.gpkg')\n",
    "if not os.path.exists('../basin_boundary'):\n",
    "    os.mkdir('../basin_boundary')\n",
    "for basin in basins:\n",
    "    gdf = read_dataframe(basin)\n",
    "    \n",
    "    # difference between ohdb_darea and grit_darea less than 20%\n",
    "    gdf['bias'] = np.abs(gdf.grit_darea - gdf.ohdb_darea_hydrosheds) / gdf.ohdb_darea_hydrosheds * 100\n",
    "    gdf1 = gdf.loc[gdf.bias<=20,:]\n",
    "    \n",
    "    # darea greater than 125 km2 to ensure at least one grid cell\n",
    "    gdf1 = gdf1.loc[gdf1.grit_darea>=125,:]\n",
    "\n",
    "    gdf1['segment_id'] = gdf1.segment_id.astype(int).astype(str)\n",
    "    gdf1['reach_id'] = gdf1.segment_id.astype(int).astype(str)\n",
    "    gdf1 = gdf1.rename(columns={'grit_darea':'gritDarea','ohdb_darea_hydrosheds':'ohdbDarea1','ohdb_darea':'ohdbDarea0'})\n",
    "    \n",
    "    # save\n",
    "    basin1 = os.path.basename(basin)\n",
    "    write_dataframe(gdf1, f'../basin_boundary/{basin1[:-5]}'+'_125km2.gpkg')\n",
    "    gdf1 = gdf1.to_crs('epsg:4326')\n",
    "    basin1 = re.sub('EPSG8857','EPSG4326',basin1)\n",
    "    write_dataframe(gdf1, f'../basin_boundary/{basin1[:-5]}'+'_125km2.shp')\n",
    "    print(basin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c336fd2",
   "metadata": {},
   "source": [
    "### transform MSWX meteo files to seperate files for each gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454426a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "1994\n",
      "1988\n",
      "1986\n",
      "1996\n",
      "1982\n",
      "1990\n",
      "1992\n",
      "1995\n",
      "1985\n",
      "1997\n",
      "1989\n",
      "1991\n",
      "1987\n",
      "1983\n",
      "1993\n",
      "1998\n",
      "2000\n",
      "2002\n",
      "2004\n",
      "2006\n",
      "2008\n",
      "2010\n",
      "2012\n",
      "1999\n",
      "2001\n",
      "2003\n",
      "2005\n",
      "2007\n",
      "2009\n",
      "2011\n",
      "2013\n",
      "2014\n",
      "2016\n",
      "2018\n",
      "2020\n",
      "2022\n",
      "2015\n",
      "2017\n",
      "2019\n",
      "2021\n",
      "2023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16710105b2948a7bf4a0fd36693d786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FUNC_MAIN DONE:   0%|          | 0/10717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "10712    None\n",
       "10713    None\n",
       "10714    None\n",
       "10715    None\n",
       "10716    None\n",
       "Length: 10717, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyogrio import read_dataframe,write_dataframe\n",
    "import geopandas as gpd\n",
    "import os,glob,sys,time,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from parallel_pandas import ParallelPandas\n",
    "from tqdm.auto import tqdm\n",
    "ParallelPandas.initialize(n_cpu=16, split_factor=16)\n",
    "\n",
    "# transform all the meteo files into separate csv files for each gauge\n",
    "def read(year, meteo = 'MSWX'):\n",
    "    if meteo == 'ERA5':\n",
    "        fname = f'../ee_era5_land/ERA5_Land_daily_meteorology_for_OHDB_10717_stations_{year}.csv'\n",
    "    elif meteo == 'MSWX':\n",
    "        fname = f'../data_mswx/MSWX_daily_meteorology_for_OHDB_10717_stations_{year}.csv'\n",
    "    df = pd.read_csv(fname).set_index('ohdb_id')\n",
    "    print(year)\n",
    "    return df\n",
    "pool = mp.Pool(8)\n",
    "df_meteo = pool.map(read, np.arange(1982, 2024).tolist())\n",
    "df_meteo = pd.concat(df_meteo, axis = 1)\n",
    "df_meteo = df_meteo.rename(columns=lambda x:x.lower())\n",
    "df_meteo = df_meteo.round(6)\n",
    "df_meteo.loc[:,df_meteo.columns.str.endswith(('_p','_tmax','_tmin','_wind'))] = df_meteo.loc[:,df_meteo.columns.str.endswith(('_p','_tmax','_tmin','_wind'))].round(2)\n",
    "df_meteo = df_meteo.reset_index()\n",
    "\n",
    "def func_main(x, meteo = 'MSWX'):\n",
    "    ohdb_id = x.ohdb_id\n",
    "    if os.path.exists(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv'):\n",
    "        try:\n",
    "            a = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "            if a.shape[0] < 15300:\n",
    "                os.remove(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "            else:\n",
    "                return\n",
    "        except:\n",
    "            os.remove(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "    x = x.drop(index=['ohdb_id'])\n",
    "    x.name = 'value'\n",
    "    y = []\n",
    "    for name in ['p','tmax','tmin','lwd','pres','relhum','spechum','swd','wind']:\n",
    "        y0 = x.loc[x.index.str.endswith('_'+name)]\n",
    "        y0.name = name\n",
    "        y0.index = y0.index.str[:8]\n",
    "        y.append(y0)\n",
    "    y = pd.concat(y, axis = 1)\n",
    "    # x = x.pivot_table(index = 'date', columns = 'meteo', values = 'value').rename(columns=lambda x:x.lower()).reset_index()\n",
    "    # x['date'] = pd.to_datetime(x.date.values)\n",
    "    # x.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv', index = False)\n",
    "    y['date'] = pd.to_datetime(y.index.values, format = '%Y%m%d')\n",
    "    y.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv', index = False)\n",
    "\n",
    "df_meteo.p_apply(func_main, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f678214",
   "metadata": {},
   "source": [
    "### connect Qmax7 and Qmin7 with meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b694fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde24a121d3543879fc85fcff266e7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<LAMBDA> DONE:   0%|          | 0/10717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ohdb_id\n",
       "OHDB_014011003     70\n",
       "OHDB_007001160     71\n",
       "OHDB_014031590     72\n",
       "OHDB_014029420     73\n",
       "OHDB_009000638     74\n",
       "                 ... \n",
       "OHDB_014005487    166\n",
       "OHDB_014005495    166\n",
       "OHDB_014005518    166\n",
       "OHDB_014024189    166\n",
       "OHDB_014000018    166\n",
       "Name: p_3, Length: 10717, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_meteo(x, name = 'Qmax7date'):\n",
    "    ohdb_id = x.xxx.values[0]\n",
    "    x = x.drop(columns=['xxx'])\n",
    "    df_meteo = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "    df_meteo['date'] = pd.to_datetime(df_meteo['date'])\n",
    "    df_meteo['ohdb_id'] = ohdb_id\n",
    "    x = x[['ohdb_id',name]].merge(df_meteo,on = 'ohdb_id')\n",
    "    x['tmp'] = (x.date - x[name]).dt.days\n",
    "    x3 = x.loc[(x.tmp>-3)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_3')\n",
    "    x7 = x.loc[(x.tmp>-7)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_7')\n",
    "    x15 = x.loc[(x.tmp>-15)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_15')\n",
    "    x30 = x.loc[(x.tmp>-30)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_30')\n",
    "    x365 = x.loc[(x.tmp>-365)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_365')\n",
    "    x = pd.concat([x3,x7,x15,x30,x365], axis = 1).reset_index()\n",
    "    return x\n",
    "\n",
    "df_flood = pd.read_csv('../data/dis_OHDB_seasonal4_Qmin7_Qmax7_1982-2023_80days.csv')\n",
    "df_flood['Qmax7date'] = pd.to_datetime(df_flood['Qmax7date'])\n",
    "df_flood['Qmin7date'] = pd.to_datetime(df_flood['Qmin7date'])\n",
    "\n",
    "df_flood['xxx'] = df_flood['ohdb_id'].values\n",
    "\n",
    "df2 = df_flood.groupby('ohdb_id').p_apply(lambda x: func_meteo(x, name = 'Qmax7date')).reset_index().drop(columns = ['level_1'])\n",
    "df2.to_csv('../data/Qmax7_seasonal4_multi_MSWX_meteo.csv', index = False)\n",
    "del df2\n",
    "\n",
    "df2 = df_flood.groupby('ohdb_id').p_apply(lambda x: func_meteo(x, name = 'Qmin7date')).reset_index().drop(columns = ['level_1'])\n",
    "df2.to_csv('../data/Qmin7_seasonal4_multi_MSWX_meteo.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c80561",
   "metadata": {},
   "source": [
    "### select OHDB gauge and calculate streamflow indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86493d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanQ(df):\n",
    "    # eliminate invalid records\n",
    "    df1 = df.loc[df.Q.apply(lambda x: not isinstance(x, str)),:]\n",
    "    df2 = df.loc[df.Q.apply(lambda x: isinstance(x, str)),:]\n",
    "    try:\n",
    "        df2 = df2.loc[df2.Q.str.match('\\d+'),:]\n",
    "    except:\n",
    "        pass\n",
    "    df = pd.concat([df1, df2])\n",
    "    df['Q'] = df.Q.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def del_unreliableQ(df):\n",
    "    '''observations less than 0 were flagged as\n",
    "        suspected, and (b) observations with more than ten consecutive\n",
    "        equal values greater than 0 were flagged as suspected'''\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').set_index('date')\n",
    "    index = pd.date_range(df.index[0], df.index[-1], freq = 'D')\n",
    "    df = df.reindex(index).fillna(0)\n",
    "    df1 = df.diff()\n",
    "    df1 = df1.where(df1==0, 1).diff()\n",
    "    start = np.where(df1.values==-1)[0]\n",
    "    end = np.where(df1.values==1)[0]\n",
    "    if len(start) == 0 or len(end) == 0:\n",
    "        # must no less than zero\n",
    "        df = df.loc[df.Q>=0,:]\n",
    "        return (df)\n",
    "    if start[0] > end[0]:\n",
    "        start = np.array([0]+start.tolist())\n",
    "    if start[-1] > end[-1]:\n",
    "        end = np.array(end.tolist()+[df1.shape[0]+10])\n",
    "    duration = end - start\n",
    "    start = start[duration>=10]\n",
    "    end = end[duration>=10]\n",
    "    del_idx = np.array([item for a,b in zip(start,end) for item in np.arange(a+1,b+2).tolist()])\n",
    "    del_idx = del_idx[del_idx<df.shape[0]]\n",
    "    if len(del_idx) > 0:\n",
    "        df.drop(df.index[del_idx], inplace = True)\n",
    "    # must no less than zero\n",
    "    df = df.loc[df.Q>=0,:]\n",
    "    return (df)\n",
    "\n",
    "def main(par, scale = 'season'):\n",
    "    ohdb_id, Darea = par\n",
    "    df = pd.read_csv(os.environ['DATA']+f'/data/OHDB/OHDB_v0.2.3/OHDB_data/discharge/daily/{ohdb_id}.csv')\n",
    "    # read\n",
    "    df = cleanQ(df)\n",
    "    # quality check\n",
    "    df = del_unreliableQ(df)\n",
    "#     # only retain records with at least 328 observations (90%) are required\n",
    "#     tmp = df.resample('Y')['Q'].agg(countDay = lambda x:x.shape[0])\n",
    "#     if tmp.loc[tmp.countDay>=328,:].shape[0] == 0:\n",
    "#         return\n",
    "#     years = tmp.loc[(tmp.countDay>=328)&(tmp.index.year>=1982),:].index.year.tolist()\n",
    "#     if tmp.loc[(tmp.countDay>=300)&(tmp.index.year==2023),:].shape[0] > 0:\n",
    "#         years = years + [2023]\n",
    "    years = np.arange(1982, 2024).tolist()\n",
    "    df = df.loc[df.index.year.isin(years),:]\n",
    "#     # only retain gauge with at least 20 years of AMS during 1982-2023\n",
    "#     if len(years) < 20:\n",
    "#         return\n",
    "    # reindex\n",
    "    newindex = pd.date_range(df.index.values[0], df.index.values[-1], freq = 'D')\n",
    "    df = df.reindex(newindex)\n",
    "    # 7-day moving average\n",
    "    df = df.rolling(7).mean().dropna()\n",
    "    df['year'] = df.index.year\n",
    "#     df['season'] = 'DJF'\n",
    "#     df.loc[(df.index.month>=3)&(df.index.month<=5),'season'] = 'MAM'\n",
    "#     df.loc[(df.index.month>=6)&(df.index.month<=8),'season'] = 'JJA'\n",
    "#     df.loc[(df.index.month>=9)&(df.index.month<=11),'season'] = 'SON'\n",
    "    df['season'] = '11-4'\n",
    "    df.loc[(df.index.month>=5)&(df.index.month<=10),'season'] = '5-10'\n",
    "    if scale == 'year':\n",
    "        # count observations and calculate Qmax7 and Qmin7 for each year\n",
    "        df1 = df.groupby('year')['Q'].agg(countDay = lambda x:x.shape[0], \n",
    "                                        Qmax7 = lambda x:x.max(),\n",
    "                                        Qmin7 = lambda x:x.min(),\n",
    "                                        Qmax7date = lambda x:x.idxmax(),\n",
    "                                        Qmin7date = lambda x:x.idxmin(),\n",
    "                                        )\n",
    "    elif scale == 'season':\n",
    "        # count observations and calculate Qmax7 and Qmin7 for each season\n",
    "        df1 = df.groupby([scale,'year'])['Q'].agg(countDay = lambda x:x.shape[0], \n",
    "                                        Qmax7 = lambda x:x.max(),\n",
    "                                        Qmin7 = lambda x:x.min(),\n",
    "                                        Qmax7date = lambda x:x.idxmax(),\n",
    "                                        Qmin7date = lambda x:x.idxmin(),\n",
    "                                        )\n",
    "#         df1 = df1.loc[df1.countDay>=80,:] # at least 80 days of records to calculate seasonal extremes\n",
    "        df1 = df1.loc[df1.countDay>=150,:] # at least 150 days of records to calculate double-seasonal extremes\n",
    "    else:\n",
    "        raise Exception('scale must be season or year')\n",
    "    df1['Qmax7date'] = pd.to_datetime(df1['Qmax7date'])\n",
    "    df1['Qmin7date'] = pd.to_datetime(df1['Qmin7date'])\n",
    "    \n",
    "    # if scale == 'season':\n",
    "    #     # keep events independent\n",
    "    #     thres = 5 + np.log(Darea * 0.386102) # thres for Qmax7\n",
    "    #     df1_Qmax = df1[['Qmax7','Qmax7date']].sort_values('Qmax7date')\n",
    "    #     df1_Qmax = df1_Qmax.loc[~(df1_Qmax.Qmax7date.diff().dt.days<thres),:]\n",
    "    #     thres = 30 # thres for Qmin7\n",
    "    #     df1_Qmin = df1[['Qmin7','Qmin7date']].sort_values('Qmin7date')\n",
    "    #     df1_Qmin = df1_Qmin.loc[~(df1_Qmin.Qmin7date.diff().dt.days<thres),:]\n",
    "    #     df1 = pd.concat([df1_Qmax, df1_Qmin], axis = 1)\n",
    "\n",
    "    # Qmax7 and Qmin7 must not be lower than zero\n",
    "    df1['Qmax7'] = df1.Qmax7.where(df1.Qmax7>=0, np.nan)\n",
    "    df1['Qmin7'] = df1.Qmin7.where(df1.Qmin7>=0, np.nan)\n",
    "\n",
    "    # # Qmax7 cannot be lower than 50% percentile of daily discharges between 1982-2023\n",
    "    # q = df.loc[df.Q>0,'Q'].quantile(0.5)\n",
    "    # df1['Qmax7'] = df1.Qmax7.where(df1.Qmax7 >= q, np.nan)\n",
    "    # # Qmin7 cannot be greater than 50% percentile of daily discharges between 1982-2023\n",
    "    # df1['Qmin7'] = df1.Qmin7.where(df1.Qmin7 <= q, np.nan)\n",
    "\n",
    "    if df1.shape[0] == 0:\n",
    "        return\n",
    "    df1 = df1.reset_index()\n",
    "    df1['ohdb_id'] = ohdb_id\n",
    "    print(ohdb_id)\n",
    "    return (df1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # if not os.path.exists('../data/OHDB_metadata_subset.csv'):\n",
    "    #     # select gauges that have good basin boundary\n",
    "    #     df = pd.read_csv('../../data/OHDB/OHDB_v0.2.3/OHDB_metadata/OHDB_metadata.csv')\n",
    "    #     df1 = []\n",
    "    #     for fname in glob.glob('../basin_boundary/GRIT*8857*'):\n",
    "    #         gdf = read_dataframe(fname, read_geometry = False)\n",
    "    #         print(gdf.shape, gdf.ohdb_id.unique().shape)\n",
    "    #         df1.append(df.loc[df.ohdb_id.isin(gdf.ohdb_id.unique()),:])\n",
    "    #     df1 = pd.concat(df1)\n",
    "    #     df1.to_csv('../OHDB_metadata_subset.csv', index = False)\n",
    "    # else:\n",
    "    df1 = pd.read_csv('../data/basin_attributes.csv')\n",
    "    print(df1.shape)\n",
    "    ohdb_ids = df1.ohdb_id.values\n",
    "    pool = mp.Pool(48)\n",
    "    pars = df1[['ohdb_id','gritDarea']].values.tolist()\n",
    "    df = pool.map(main, pars)\n",
    "    df = pd.concat(df)\n",
    "    df.to_csv('../data/dis_OHDB_seasonal_Qmin7_Qmax7_1982-2023.csv', index = False)\n",
    "    print(df.Qmin7.isna().sum(), df.Qmax7.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b7546",
   "metadata": {},
   "source": [
    "### subset basin boundary files again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec385a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dis_OHDB_Qmin7_Qmax7_1982-2023.csv')\n",
    "ohdb_ids = df.ohdb_id.unique()\n",
    "for fname in glob.glob('../basin_boundary/GRIT*8857*'):\n",
    "    gdf = read_dataframe(fname)\n",
    "    gdf = gdf.loc[gdf.ohdb_id.isin(ohdb_ids),:]\n",
    "    write_dataframe(gdf, fname[:-5]+'_subset.gpkg')\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b20a1",
   "metadata": {},
   "source": [
    "### use catch_mean_GLHYMPS_GLiM.py to get catchment average subsurface characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff8dce",
   "metadata": {},
   "source": [
    "### calculate number of upstream dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dam = read_dataframe('../../data/geography/GDAT_data_v1/data/GDAT_v1_dams.shp')\n",
    "gdf_dam = gdf_dam.to_crs('espg:8857')\n",
    "gdf = read_dataframe('../basin_boundary/GRIT_full_catchment_all_EPSG8857_simplify_final_125km2_subset.gpkg')\n",
    "join = gpd.overlay(gdf_dam, gdf)\n",
    "join = join.groupby('ohdb_id')['Feature_ID'].count().rename(columns={'Feature_ID':'dam_num'})\n",
    "join = join.reindex(gdf.ohdb_id.values).fillna(0).reset_index()\n",
    "join.to_csv('../geography/dam_num.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c7652",
   "metadata": {},
   "source": [
    "### extract average meteorological conditions in the past 7 days preceding Qmax7 and Qmin7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617a3d6",
   "metadata": {},
   "source": [
    "### merge basin attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffa8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob('../geography/*csv')\n",
    "df_all = []\n",
    "for fname in fnames:\n",
    "    df = pd.read_csv(fname)\n",
    "    if df.shape[0] == 1:\n",
    "        df = df.T\n",
    "    if df.shape[0] == 10717:\n",
    "        df = df.set_index('ohdb_id')\n",
    "    name = os.path.basename(fname).split('_')[0]\n",
    "    if '0-5cm' in fname:\n",
    "        name = name + '_layer1'\n",
    "    elif '5-15cm' in fname:\n",
    "        name = name + '_layer2'\n",
    "    elif '15-30cm' in fname:\n",
    "        name = name + '_layer3'\n",
    "    elif '30-60cm' in fname:\n",
    "        name = name + '_layer4'\n",
    "    elif '60-100cm' in fname:\n",
    "        name = name + '_layer5'\n",
    "    elif '100-200cm' in fname:\n",
    "        name = name + '_layer6'\n",
    "    if df.shape[1] == 1:\n",
    "        df.columns = [name]\n",
    "    df_all.append(df)\n",
    "df_all = pd.concat(df_all, axis = 1)\n",
    "df_all = df_all.loc[df_all.index.str.contains('OHDB'),:].reset_index().rename(columns={'index':'ohdb_id'})\n",
    "\n",
    "# merge metadata\n",
    "df_meta = pd.read_csv('../OHDB_metadata_subset.csv')\n",
    "df_all = df_all.merge(df_meta, on = 'ohdb_id')\n",
    "\n",
    "df_all.to_csv('../basin_attributes.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78212bd",
   "metadata": {},
   "source": [
    "### use GDAT reservoir area / catchment area to indicate the impact of reservoir regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb162d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_res = read_dataframe('../../data/geography/GDAT_data_v1/data/GDAT_v1_catchments.shp').to_crs('epsg:8857')\n",
    "gdf_dam = read_dataframe('../../data/geography/GDAT_data_v1/data/GDAT_v1_dams.shp').to_crs('epsg:8857')\n",
    "gdf_basin = read_dataframe('../basin_boundary/GRIT_full_catchment_all_EPSG8857_simplify_final_125km2_subset.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52193d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = gpd.sjoin(gdf_basin, gdf_dam)\n",
    "gdf_res['darea'] = gdf_res.area / 1000000\n",
    "inter = inter.merge(gdf_res[['Feature_ID','Dam_Name','darea']], on = ['Feature_ID','Dam_Name'])\n",
    "inter.loc[inter.Year_Fin=='BLANK','Year_Fin'] = None\n",
    "inter['Year_Fin'] = pd.to_numeric(inter['Year_Fin'])\n",
    "\n",
    "inter.loc[inter.Year_Const.isna(),'Year_Const'] = np.nan\n",
    "inter.loc[~inter.Year_Const.isna(),'Year_Const'] = inter.loc[~inter.Year_Const.isna(),'Year_Const'].str[:4].astype(int)\n",
    "inter['year'] = inter[['Year_Fin','Year_Const']].min(axis=1)\n",
    "inter = inter.groupby(['ohdb_id','gritDarea']).apply(lambda x:pd.Series([\n",
    "    x.darea.sum(),\n",
    "    x.year.mean(),\n",
    "    x.Main_P_Map.mode().values[0] if len(x.Main_P_Map.mode().values)>=1 else 'Hydroelectricity'\n",
    "], index = ['res_darea_normalize','Year_ave','Main_Purpose_mode'])).reset_index()\n",
    "inter['Year_ave'] = inter['Year_ave'].fillna(2000)\n",
    "inter['Main_Purpose_mode'] = inter['Main_Purpose_mode'].fillna('Hydroelectricity')\n",
    "inter['res_darea_normalize'] = inter.res_darea_normalize / inter.gritDarea\n",
    "inter.to_csv('../data/dam_impact.csv', index = False)\n",
    "print(inter.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2851a",
   "metadata": {},
   "source": [
    "### add country ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('../data/basin_attributes.csv')\n",
    "gdf_gauge = gpd.GeoDataFrame(data = df, geometry = gpd.points_from_xy(df.ohdb_longitude, df.ohdb_latitude), crs = 'epsg:4326').to_crs('epsg:8857')\n",
    "gdf_country = read_dataframe('../geography/ne_10m_admin_0_countries_lakes.shp').to_crs('epsg:8857')\n",
    "fig, ax = plt.subplots()\n",
    "gdf_country.plot(ax = ax, column = 'SOVEREIGNT')\n",
    "gdf_gauge.plot(ax = ax, zorder = 3, markersize = .1, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/basin_attributes.csv')\n",
    "df = df.loc[:,~df.columns.str.contains('country')]\n",
    "df = df.loc[:,~df.columns.str.contains('SOVEREIGNT')]\n",
    "inter = gpd.sjoin(gdf_gauge, gdf_country[['geometry','SOVEREIGNT']], how = 'left')\n",
    "inter1  = inter.loc[~inter.SOVEREIGNT.isna(),:]\n",
    "gdf_gauge2 = gdf_gauge.loc[~gdf_gauge.ohdb_id.isin(inter1.ohdb_id.values),:]\n",
    "inter2 = []\n",
    "for i in range(gdf_gauge2.shape[0]):\n",
    "    tmp = gpd.sjoin_nearest(gdf_gauge2.iloc[[i],:], gdf_country[['geometry','SOVEREIGNT']], how = 'left')\n",
    "    inter2.append(tmp)\n",
    "inter2 = pd.concat(inter2)\n",
    "inter = pd.concat([inter1, inter2])\n",
    "print(inter.shape, inter.ohdb_id.unique().shape, inter.shape, gdf_gauge.shape)\n",
    "df = df.merge(inter[['ohdb_id','SOVEREIGNT']], on = 'ohdb_id').rename(columns = {'SOVEREIGNT':'country'})\n",
    "\n",
    "df.to_csv('../data/basin_attributes.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e37048",
   "metadata": {},
   "source": [
    "### calculate annual average and std of Tmax and Tmin as basin attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "fnames = glob.glob('../data_mswx/*meteo*')\n",
    "def func(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df.set_index('ohdb_id')\n",
    "    df = df.loc[:,(df.columns.str.contains('Tmax'))|(df.columns.str.contains('Tmin'))]\n",
    "    return (df)\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(8)\n",
    "df = pool.map(func, fnames)\n",
    "df = pd.concat(df, axis = 1)\n",
    "df_Tmax = pd.concat([\n",
    "    df.loc[:,df.columns.str.contains('Tmax')].mean(axis = 1),\n",
    "    df.loc[:,df.columns.str.contains('Tmax')].std(axis = 1)\n",
    "], axis = 1)\n",
    "df_Tmin = pd.concat([\n",
    "    df.loc[:,df.columns.str.contains('Tmin')].mean(axis = 1),\n",
    "    df.loc[:,df.columns.str.contains('Tmin')].std(axis = 1)\n",
    "], axis = 1)\n",
    "\n",
    "df_Tmax.columns = ['tmax_ave','tmax_std']\n",
    "df_Tmin.columns = ['tmin_ave','tmin_std']\n",
    "df_Tmax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b122f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tmax.columns = ['tmax_ave','tmax_std']\n",
    "df_Tmin.columns = ['tmin_ave','tmin_std']\n",
    "df_attr = pd.read_csv('../data/basin_attributes.csv').set_index('ohdb_id')\n",
    "df_attr = df_attr.loc[:,~df_attr.columns.str.contains('tmax')]\n",
    "df_attr = df_attr.loc[:,~df_attr.columns.str.contains('tmin')]\n",
    "df_attr = pd.concat([df_Tmax, df_attr, df_Tmin], axis = 1).reset_index()\n",
    "df_attr.to_csv('../data/basin_attributes.csv', index = False)\n",
    "\n",
    "df_attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python merge_dataset_for_modeling.py Qmin7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/*final*season*multi*MSWX* -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Qmin7_final_dataset_seasonal4_multi_MSWX_meteo.csv')\n",
    "df.loc[df.tmax_3<0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!time python tmp.py ../data/Qmin7_final_dataset_seasonal_multi_MSWX_meteo.csv rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55703c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
