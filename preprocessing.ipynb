{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5c753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyogrio import read_dataframe,write_dataframe\n",
    "import geopandas as gpd\n",
    "import os,glob,sys,time,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from parallel_pandas import ParallelPandas\n",
    "from tqdm.auto import tqdm\n",
    "ParallelPandas.initialize(n_cpu=24, split_factor=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc330ab3",
   "metadata": {},
   "source": [
    "### get basin boundary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bff36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = glob.glob('../../data/GRIT/full_catchment/GRIT_full_catchment_*_EPSG8857_simplify_final.gpkg')\n",
    "if not os.path.exists('../basin_boundary'):\n",
    "    os.mkdir('../basin_boundary')\n",
    "for basin in basins:\n",
    "    gdf = read_dataframe(basin)\n",
    "    \n",
    "    # difference between ohdb_darea and grit_darea less than 20%\n",
    "    gdf['bias'] = np.abs(gdf.grit_darea - gdf.ohdb_darea_hydrosheds) / gdf.ohdb_darea_hydrosheds * 100\n",
    "    gdf1 = gdf.loc[gdf.bias<=20,:]\n",
    "    \n",
    "    # darea greater than 125 km2 to ensure at least one grid cell\n",
    "    gdf1 = gdf1.loc[gdf1.grit_darea>=125,:]\n",
    "\n",
    "    gdf1['segment_id'] = gdf1.segment_id.astype(int).astype(str)\n",
    "    gdf1['reach_id'] = gdf1.segment_id.astype(int).astype(str)\n",
    "    gdf1 = gdf1.rename(columns={'grit_darea':'gritDarea','ohdb_darea_hydrosheds':'ohdbDarea1','ohdb_darea':'ohdbDarea0'})\n",
    "    \n",
    "    # save\n",
    "    basin1 = os.path.basename(basin)\n",
    "    write_dataframe(gdf1, f'../basin_boundary/{basin1[:-5]}'+'_125km2.gpkg')\n",
    "    gdf1 = gdf1.to_crs('epsg:4326')\n",
    "    basin1 = re.sub('EPSG8857','EPSG4326',basin1)\n",
    "    write_dataframe(gdf1, f'../basin_boundary/{basin1[:-5]}'+'_125km2.shp')\n",
    "    print(basin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258d99b",
   "metadata": {},
   "source": [
    "### connect OHDB gauge with GLAKES polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyogrio import read_dataframe\n",
    "import geopandas as gpd\n",
    "gdf_basin = read_dataframe('../basin_boundary/GRIT_full_catchment_all_EPSG8857_simplify_final_125km2_subset.gpkg')\n",
    "df_attr = pd.read_csv('../data/basin_attributes.csv')\n",
    "gdf_basin = gdf_basin.merge(df_attr[['ohdb_id','ohdb_longitude','ohdb_latitude']], on = 'ohdb_id')\n",
    "p = gpd.GeoDataFrame(data = gdf_basin[['ohdb_id','ohdb_longitude','ohdb_latitude']], \n",
    "                     geometry = gpd.points_from_xy(gdf_basin.ohdb_longitude.values,gdf_basin.ohdb_latitude.values),\n",
    "                     crs='epsg:4326')\n",
    "p = p.to_crs('epsg:8857')\n",
    "p['x_sta'] = p.geometry.centroid.x\n",
    "p['y_sta'] = p.geometry.centroid.y\n",
    "gdf_basin = gdf_basin.merge(p[['ohdb_id','x_sta','y_sta']], on = 'ohdb_id')\n",
    "\n",
    "import glob\n",
    "connect_all = []\n",
    "for fname in glob.glob('../../data/geography/GLAKES/GLAKES/*shp'):\n",
    "    gdf = read_dataframe(fname, columns = ['Lake_id','Area_bound','Lat','Lon'])\n",
    "    p = gpd.GeoDataFrame(data = gdf[['Lake_id','Lat','Lon']], geometry = gpd.points_from_xy(gdf.Lon.values,gdf.Lat.values),crs='epsg:4326')\n",
    "    p = p.to_crs('epsg:8857')\n",
    "    p['x_res'] = p.geometry.centroid.x\n",
    "    p['y_res'] = p.geometry.centroid.y\n",
    "    gdf = gdf.to_crs('epsg:8857')\n",
    "    gdf = gdf.merge(p[['Lake_id','x_res','y_res']], on = 'Lake_id')\n",
    "    connect = gpd.sjoin(gdf_basin, gdf)\n",
    "    connect = connect.groupby('ohdb_id').apply(\n",
    "        lambda x:pd.Series([x.Lake_id.count(),\n",
    "                            x.Area_bound.sum(),\n",
    "                            np.min(np.sqrt((x.x_res-x.x_sta)**2+(x.y_res-x.y_sta)**2))/1000,\n",
    "                            x.iloc[np.argmin((x.x_res-x.x_sta)**2+(x.y_res-x.y_sta)**2),:].Area_bound,\n",
    "                           ], \n",
    "        index = ['num_lake','area_all_lakes','min_dis_km','area_nearest'])\n",
    "    )\n",
    "    connect_all.append(connect)\n",
    "    print(fname)\n",
    "connect_all = pd.concat(connect_all).reset_index()\n",
    "connect_all = connect_all.groupby('ohdb_id').apply(\n",
    "    lambda x:pd.Series([\n",
    "        x.num_lake.sum(),\n",
    "        x.area_all_lakes.sum(),\n",
    "        x.min_dis_km.min(),\n",
    "        x.iloc[np.argmin(x.min_dis_km),:].area_nearest,\n",
    "    ], index = ['num_lake','area_all_lakes','min_dis_km','area_nearest'])\n",
    ").reset_index()\n",
    "connect_all.to_csv('../data/basin_connect_GLAKES.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c80561",
   "metadata": {},
   "source": [
    "### select OHDB gauge and calculate streamflow indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86493d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10717, 9)\n",
      "Finish processing, then start merging\n",
      "Finish merging (1536373, 11)\n",
      "0 0 (10586,)\n"
     ]
    }
   ],
   "source": [
    "def cleanQ(df):\n",
    "    # eliminate invalid records\n",
    "    df1 = df.loc[df.Q.apply(lambda x: not isinstance(x, str)),:]\n",
    "    df2 = df.loc[df.Q.apply(lambda x: isinstance(x, str)),:]\n",
    "    try:\n",
    "        df2 = df2.loc[df2.Q.str.match('\\d+'),:]\n",
    "    except:\n",
    "        pass\n",
    "    df = pd.concat([df1, df2])\n",
    "    df['Q'] = df.Q.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def del_unreliableQ(df):\n",
    "    '''\n",
    "    all records are rounded to three decimal places\n",
    "    observations less than 0 were flagged as suspected\n",
    "    observations with more than ten consecutive equal values greater than 0 were flagged as suspected\n",
    "    '''\n",
    "    df = df.loc[df.Q>=0,:].reset_index()\n",
    "    df['Q'] = df['Q'].round(3)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').set_index('date')\n",
    "    index = pd.date_range(df.index[0], df.index[-1], freq = 'D')\n",
    "    df = df.reindex(index)\n",
    "    df1 = df.diff()\n",
    "    df1 = df1.where(df1==0, 1).diff()\n",
    "    start = np.where(df1.values==-1)[0]\n",
    "    end = np.where(df1.values==1)[0]\n",
    "    if len(start) == 0 or len(end) == 0:\n",
    "        # must no less than zero\n",
    "        df = df.loc[df.Q>=0,:]\n",
    "        return (df)\n",
    "    if start[0] > end[0]:\n",
    "        start = np.array([0]+start.tolist())\n",
    "    if start[-1] > end[-1]:\n",
    "        end = np.array(end.tolist()+[df1.shape[0]+10])\n",
    "    duration = end - start\n",
    "    start = start[duration>=10]\n",
    "    end = end[duration>=10]\n",
    "    del_idx = np.array([item for a,b in zip(start,end) for item in np.arange(a+1,b+2).tolist()])\n",
    "    del_idx = del_idx[del_idx<df.shape[0]]\n",
    "     # identical values should be greater than zero\n",
    "    del_idx = [s for s in del_idx if df.iloc[s,:].Q > 0]\n",
    "    if len(del_idx) > 0:\n",
    "        df.drop(df.index[del_idx], inplace = True)\n",
    "    # must no less than zero\n",
    "    df = df.loc[df.Q>=0,:]\n",
    "    return (df)\n",
    "\n",
    "def del_outlierQ(df):\n",
    "    '''\n",
    "        Based on a previously suggested approach for evaluating temperature series (Klein Tank et al., 2009), \n",
    "        daily streamflow values are declared as outliers if values of log (Q+0.01) are larger or smaller than \n",
    "        the mean value of log (Q+0.01) plus or minus 6 times the standard deviation of log (Q+0.01) computed for \n",
    "        that calendar day for the entire length of the series. The mean and standard deviation are computed for \n",
    "        a 5-day window centred on the calendar day to ensure that a sufficient amount of data is considered. \n",
    "        The log-transformation is used to account for the skewness of the distribution of daily streamflow values \n",
    "        and 0.01 was added because the logarithm of zero is undefined. Outliers are flagged as suspect. \n",
    "        The rationale underlying this rule is that unusually large or small values are often associated with observational issues. \n",
    "        The 6 standard-deviation threshold is a compromise, aiming at screening out outliers that could come from \n",
    "        instrument malfunction, while not flagging extreme floods or low flows.\n",
    "    '''\n",
    "    df['logQ'] = np.log(df['Q']+0.01)\n",
    "    df['doy'] = df.index.dayofyear\n",
    "    df['year'] = df.index.year\n",
    "    df = df.pivot_table(index = 'doy', columns = 'year', values = 'logQ').reset_index()\n",
    "    def tmp(x0):\n",
    "        x = np.arange(x0-2, x0+3) \n",
    "        x = np.where(x <= 0, x + 366, x)\n",
    "        x = np.where(x > 366, x - 366, x)\n",
    "        s = df.loc[df.doy.isin(x),:].drop(columns=['doy']).values.flatten()\n",
    "        ave = np.nanmean(s)\n",
    "        std = np.nanstd(s)\n",
    "        low = ave - std * 6\n",
    "        upp = ave + std * 6\n",
    "        return (x0, low, upp)\n",
    "    thres = list(map(tmp, np.arange(1, 367)))\n",
    "    thres = pd.DataFrame(data = np.array(thres), columns = ['doy','low','upp'])\n",
    "    df = df.merge(thres, on = 'doy').set_index('doy')\n",
    "    df.iloc[:,:(df.shape[1]-2)] = df.iloc[:,:(df.shape[1]-2)].where(df.iloc[:,:(df.shape[1]-2)].lt(df['upp'], axis=0))\n",
    "    df.iloc[:,:(df.shape[1]-2)] = df.iloc[:,:(df.shape[1]-2)].where(df.iloc[:,:(df.shape[1]-2)].gt(df['low'], axis=0))\n",
    "    df = df.drop(columns = ['low','upp']).stack().reset_index(name='logQ')\n",
    "    df['Q'] = np.exp(df['logQ']) - 0.01\n",
    "    df['Q'] = np.where(df['Q'].abs()<1e-6, 0, df['Q'])\n",
    "    df['date'] = pd.to_datetime(df['level_1'].astype(str) + '-' + df['doy'].astype(str), format='%Y-%j')\n",
    "    df = df[['date','Q']].sort_values('date').set_index('date')\n",
    "    return df\n",
    "\n",
    "def remove_periodic_zero(df):\n",
    "    df['year'] = df.index.year\n",
    "    df['doy'] = df.index.dayofyear\n",
    "    df1 = df.pivot_table(index = 'year', columns = 'doy', values = 'Q').fillna(0)\n",
    "    s = (df1==0).all()\n",
    "    s.name = 'periodic'\n",
    "    df = df.merge(pd.DataFrame(s).reset_index(), on = 'doy')\n",
    "    df.loc[df.periodic==True,'Q'] = np.nan\n",
    "    df['date'] = pd.to_datetime(df['year'] * 1000 + df['doy'], format='%Y%j')\n",
    "    df = df[['date','Q']].set_index('date')\n",
    "    return df\n",
    "\n",
    "def main(par, scale = 'season'):\n",
    "    ohdb_id, Darea = par\n",
    "    df = pd.read_csv(os.environ['DATA']+f'/data/OHDB/OHDB_v0.2.3/OHDB_data/discharge/daily/{ohdb_id}.csv')\n",
    "    # read\n",
    "    df = cleanQ(df)\n",
    "    # quality check\n",
    "    df = del_unreliableQ(df)\n",
    "    # delete outliers\n",
    "    df = del_outlierQ(df)\n",
    "    # periodic zero not included\n",
    "    df = remove_periodic_zero(df)\n",
    "#     # only retain records with at least 328 observations (90%) are required\n",
    "#     tmp = df.resample('YE')['Q'].agg(countDay = lambda x:x.shape[0])\n",
    "#     if tmp.loc[tmp.countDay>=328,:].shape[0] == 0:\n",
    "#         return\n",
    "#     years = tmp.loc[(tmp.countDay>=328)&(tmp.index.year>=1982),:].index.year.tolist()\n",
    "#     if tmp.loc[(tmp.countDay>=300)&(tmp.index.year==2023),:].shape[0] > 0:\n",
    "#         years = years + [2023]\n",
    "    df = df.loc[(df.index.year>=1981)&(df.index.year<=2023),:]\n",
    "    years = df.index.year.unique()\n",
    "    # only retain gauge with at least 20 years of AMS during 1982-2023\n",
    "    if len(years[years>=1982]) < 20:\n",
    "        return\n",
    "    df = df.loc[df.index.year.isin(years),:]\n",
    "    # filter gauge with coefficient of variability less than 0.05\n",
    "    if df.shape[0] == 0 or df.Q.std() / df.Q.mean() < 0.05:\n",
    "        return\n",
    "    \n",
    "    # reindex\n",
    "    newindex = pd.date_range(df.index.values[0], df.index.values[-1], freq = 'D')\n",
    "    df = df.reindex(newindex)\n",
    "    \n",
    "    # 7-day moving average\n",
    "    df['Q7'] = df.rolling(7).mean().dropna()\n",
    "    df['year'] = df.index.year\n",
    "    \n",
    "    df['winter_year'] = df.index.year\n",
    "    df.loc[df.index.month == 12, 'winter_year'] += 1\n",
    "    \n",
    "    df['season'] = 'DJF'\n",
    "    df.loc[(df.index.month>=3)&(df.index.month<=5),'season'] = 'MAM'\n",
    "    df.loc[(df.index.month>=6)&(df.index.month<=8),'season'] = 'JJA'\n",
    "    df.loc[(df.index.month>=9)&(df.index.month<=11),'season'] = 'SON'\n",
    "    df = df.dropna()\n",
    "\n",
    "    #     df['season'] = '11-4'\n",
    "    #     df.loc[(df.index.month>=5)&(df.index.month<=10),'season'] = '5-10'\n",
    "    if scale == 'year':\n",
    "        # count observations and calculate Qmax7 and Qmin7 for each year\n",
    "        df1 = df.groupby('year').agg(countDay = ('Q',lambda x:x.shape[0]), \n",
    "                                        Qmax7 = ('Q7',lambda x:x.max()),\n",
    "                                        Qmax = ('Q',lambda x:x.max()),\n",
    "                                        Qmin7 = ('Q7',lambda x:x.min()),\n",
    "                                        Q7zeroCount = ('Q7',lambda x:np.sum(x==0)),\n",
    "                                        Qmaxdate = ('Q',lambda x:x.idxmax()),\n",
    "                                        Qmax7date = ('Q7',lambda x:x.idxmax()),\n",
    "                                        Qmin7date = ('Q7',lambda x:x.idxmin()),\n",
    "                                        )\n",
    "    elif scale == 'season':\n",
    "        # count observations and calculate Qmax7 and Qmin7 for each season\n",
    "        df1 = df.groupby([scale,'winter_year']).agg(countDay = ('Q',lambda x:x.shape[0]), \n",
    "                                        Qmax7 = ('Q7',lambda x:x.max()),\n",
    "                                        Qmax = ('Q',lambda x:x.max()),\n",
    "                                        Qmin7 = ('Q7',lambda x:x.min()),\n",
    "                                        Q7zeroCount = ('Q7',lambda x:np.sum(x==0)),\n",
    "                                        Qmaxdate = ('Q',lambda x:x.idxmax()),\n",
    "                                        Qmax7date = ('Q7',lambda x:x.idxmax()),\n",
    "                                        Qmin7date = ('Q7',lambda x:x.idxmin()),\n",
    "                                        )\n",
    "        df1 = df1.loc[df1.countDay>=80,:] # at least 80 days of records to calculate seasonal extremes\n",
    "#         df1 = df1.loc[df1.countDay>=150,:] # at least 150 days of records to calculate double-seasonal extremes\n",
    "    else:\n",
    "        raise Exception('scale must be season or year')\n",
    "    df1['Qmax7date'] = pd.to_datetime(df1['Qmax7date'])\n",
    "    df1['Qmin7date'] = pd.to_datetime(df1['Qmin7date'])\n",
    "    df1['Qmaxdate'] = pd.to_datetime(df1['Qmaxdate'])\n",
    "    \n",
    "    # Q must not be lower than zero\n",
    "    df1['Qmax7'] = df1.Qmax7.where(df1.Qmax7>=0, np.nan)\n",
    "    df1['Qmin7'] = df1.Qmin7.where(df1.Qmin7>=0, np.nan)\n",
    "    df1['Qmax'] = df1.Qmax.where(df1.Qmax>=0, np.nan)\n",
    "\n",
    "    if df1.shape[0] == 0:\n",
    "        return\n",
    "    df1 = df1.reset_index()\n",
    "    df1['ohdb_id'] = ohdb_id\n",
    "    return (df1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # if not os.path.exists('../data/OHDB_metadata_subset.csv'):\n",
    "    #     # select gauges that have good basin boundary\n",
    "    #     df = pd.read_csv('../../data/OHDB/OHDB_v0.2.3/OHDB_metadata/OHDB_metadata.csv')\n",
    "    #     df1 = []\n",
    "    #     for fname in glob.glob('../basin_boundary/GRIT*8857*'):\n",
    "    #         gdf = read_dataframe(fname, read_geometry = False)\n",
    "    #         print(gdf.shape, gdf.ohdb_id.unique().shape)\n",
    "    #         df1.append(df.loc[df.ohdb_id.isin(gdf.ohdb_id.unique()),:])\n",
    "    #     df1 = pd.concat(df1)\n",
    "    #     df1.to_csv('../OHDB_metadata_subset.csv', index = False)\n",
    "    # else:\n",
    "    gdf = read_dataframe('../basin_boundary/GRIT_full_catchment_all_EPSG8857_simplify_final_125km2_subset.gpkg')\n",
    "    print(gdf.shape)\n",
    "    ohdb_ids = gdf.ohdb_id.values\n",
    "    pool = mp.Pool(48)\n",
    "    pars = gdf[['ohdb_id','gritDarea']].values.tolist()\n",
    "    df = pool.map(main, pars)\n",
    "    print('Finish processing, then start merging')\n",
    "    df = pd.concat(df)\n",
    "    print('Finish merging', df.shape)\n",
    "    df.to_csv('../data/dis_OHDB_seasonal4_Qmin7_Qmax7_1982-2023.csv', index = False)\n",
    "    print(df.Qmin7.isna().sum(), df.Qmax7.isna().sum(), df.ohdb_id.unique().shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c336fd2",
   "metadata": {},
   "source": [
    "### transform MSWX meteo files to seperate files for each gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454426a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyogrio import read_dataframe,write_dataframe\n",
    "import geopandas as gpd\n",
    "import os,glob,sys,time,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from parallel_pandas import ParallelPandas\n",
    "from tqdm.auto import tqdm\n",
    "ParallelPandas.initialize(n_cpu=16, split_factor=16)\n",
    "\n",
    "# transform all the meteo files into separate csv files for each gauge\n",
    "def read(year, meteo = 'MSWX'):\n",
    "    if meteo == 'ERA5':\n",
    "        fname = f'../ee_era5_land/ERA5_Land_daily_meteorology_for_OHDB_10717_stations_{year}.csv'\n",
    "    elif meteo == 'MSWX':\n",
    "        fname = f'../data_mswx/MSWX_daily_meteorology_for_OHDB_10717_stations_{year}.csv'\n",
    "    df = pd.read_csv(fname).set_index('ohdb_id')\n",
    "    print(year)\n",
    "    return df\n",
    "pool = mp.Pool(8)\n",
    "df_meteo = pool.map(read, np.arange(1982, 2024).tolist())\n",
    "df_meteo = pd.concat(df_meteo, axis = 1)\n",
    "df_meteo = df_meteo.rename(columns=lambda x:x.lower())\n",
    "df_meteo = df_meteo.round(6)\n",
    "df_meteo.loc[:,df_meteo.columns.str.endswith(('_p','_tmax','_tmin','_wind'))] = df_meteo.loc[:,df_meteo.columns.str.endswith(('_p','_tmax','_tmin','_wind'))].round(2)\n",
    "df_meteo = df_meteo.reset_index()\n",
    "\n",
    "def func_main(x, meteo = 'MSWX'):\n",
    "    ohdb_id = x.ohdb_id\n",
    "    if os.path.exists(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv'):\n",
    "        try:\n",
    "            a = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "            if a.shape[0] < 15300:\n",
    "                os.remove(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "            else:\n",
    "                return\n",
    "        except:\n",
    "            os.remove(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "    x = x.drop(index=['ohdb_id'])\n",
    "    x.name = 'value'\n",
    "    y = []\n",
    "    for name in ['p','tmax','tmin','lwd','pres','relhum','spechum','swd','wind']:\n",
    "        y0 = x.loc[x.index.str.endswith('_'+name)]\n",
    "        y0.name = name\n",
    "        y0.index = y0.index.str[:8]\n",
    "        y.append(y0)\n",
    "    y = pd.concat(y, axis = 1)\n",
    "    # x = x.pivot_table(index = 'date', columns = 'meteo', values = 'value').rename(columns=lambda x:x.lower()).reset_index()\n",
    "    # x['date'] = pd.to_datetime(x.date.values)\n",
    "    # x.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv', index = False)\n",
    "    y['date'] = pd.to_datetime(y.index.values, format = '%Y%m%d')\n",
    "    y.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv', index = False)\n",
    "\n",
    "df_meteo.p_apply(func_main, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bee83",
   "metadata": {},
   "source": [
    "### transform MSWEP and GLEAM files to separate files for each gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyogrio import read_dataframe,write_dataframe\n",
    "import geopandas as gpd\n",
    "import os,glob,sys,time,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from parallel_pandas import ParallelPandas\n",
    "from tqdm.auto import tqdm\n",
    "ParallelPandas.initialize(n_cpu=16, split_factor=16)\n",
    "\n",
    "# transform all the meteo files into separate csv files for each gauge\n",
    "def read(par):\n",
    "    year, meteo = par\n",
    "    if meteo == 'MSWEP':\n",
    "        df = pd.read_csv(f'../data/GRIT_catch_ave_pr_MSWEP_{year}.csv').set_index('ohdb_id')\n",
    "    elif meteo == 'E':\n",
    "        df = pd.read_csv(f'../gleam_data/E_{year}_GLEAM_v4.1a.csv').set_index('ohdb_id')\n",
    "    elif meteo == 'SM':\n",
    "        df = pd.read_csv(f'../gleam_data/SMrz_{year}_GLEAM_v4.1a.csv').set_index('ohdb_id')\n",
    "    if meteo == 'MSWEP':\n",
    "        df = df / 10\n",
    "    print(year)\n",
    "    return df\n",
    "pool = mp.Pool(8)\n",
    "df_mswep = pool.map(read, [(a,'MSWEP') for a in np.arange(1982, 2024).tolist()])\n",
    "df_mswep = pd.concat(df_mswep, axis = 1)\n",
    "\n",
    "df_e = pool.map(read, [(a,'E') for a in np.arange(1982, 2024).tolist()])\n",
    "df_e = pd.concat(df_e, axis = 1)\n",
    "\n",
    "df_sm = pool.map(read, [(a,'SM') for a in np.arange(1982, 2024).tolist()])\n",
    "df_sm = pd.concat(df_sm, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_main(x, meteo = 'MSWX'):\n",
    "    ohdb_id = x.ohdb_id\n",
    "    if os.path.exists(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv'):\n",
    "        try:\n",
    "            a = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "            if a.shape[0] < 15300:\n",
    "                os.remove(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "            else:\n",
    "                return\n",
    "        except:\n",
    "            os.remove(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "    x = x.drop(index=['ohdb_id'])\n",
    "    x.name = 'value'\n",
    "    y = []\n",
    "    for name in ['p','tmax','tmin','lwd','pres','relhum','spechum','swd','wind']:\n",
    "        y0 = x.loc[x.index.str.endswith('_'+name)]\n",
    "        y0.name = name\n",
    "        y0.index = y0.index.str[:8]\n",
    "        y.append(y0)\n",
    "    y = pd.concat(y, axis = 1)\n",
    "    # x = x.pivot_table(index = 'date', columns = 'meteo', values = 'value').rename(columns=lambda x:x.lower()).reset_index()\n",
    "    # x['date'] = pd.to_datetime(x.date.values)\n",
    "    # x.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv', index = False)\n",
    "    y['date'] = pd.to_datetime(y.index.values, format = '%Y%m%d')\n",
    "    y.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv', index = False)\n",
    "\n",
    "df_meteo.p_apply(func_main, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d5469",
   "metadata": {},
   "source": [
    "### connect MSWX with snowmelt and snowfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def snow_to_each_file(x):\n",
    "#     ohdb_id = x.name\n",
    "# #     tmp = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "#     x = x.reset_index().rename(columns={'time':'date',ohdb_id:'snowmelt'})\n",
    "# #     y = tmp.merge(x, on = 'date')\n",
    "#     x.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_snow.csv', index = False)\n",
    "# for name in ['snow']:\n",
    "#     fnames = glob.glob(f'../data_mswx/GRIT_catch_ave_{name}_[0-9]*csv')\n",
    "#     df = pd.concat([pd.read_csv(fname).set_index('time') for fname in fnames], axis = 0)\n",
    "#     df.p_agg(snow_to_each_file)\n",
    "\n",
    "import multiprocessing as mp\n",
    "def func_merge_snow(ohdb_id):\n",
    "    df = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "    snowfall = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_snowfall.csv')\n",
    "    snowmelt = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_snowmelt.csv')\n",
    "    snow = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_snow.csv')\n",
    "    df = df.merge(snowfall.rename(columns={'snowmelt':'snowfall'}), on = 'date')\n",
    "    df = df.merge(snowmelt, on = 'date')\n",
    "    df = df.merge(snow.rename(columns={'snowmelt':'snow'}), on = 'date')\n",
    "    df.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_new.csv', index = False)\n",
    "    print(ohdb_id)\n",
    "\n",
    "import glob,os\n",
    "ohdb_ids = [os.path.basename(fname).split('.')[0] for fname in glob.glob('../data_mswx/mswx_each_basin/OHDB_*[0-9].csv')]\n",
    "print(len(ohdb_ids))\n",
    "pool = mp.Pool(16)\n",
    "pool.map(func_merge_snow, ohdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f678214",
   "metadata": {},
   "source": [
    "### connect Qmax7 and Qmin7 with meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b694fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb587bcab8440519bc4bb9fcdb76259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<LAMBDA> DONE:   0%|          | 0/10520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f48b5d1a9e49339927a3facb09fb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<LAMBDA> DONE:   0%|          | 0/10520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6b5d98c5b748de94af73bea98cac0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<LAMBDA> DONE:   0%|          | 0/10520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def func_meteo(x, name = 'Qmax7date'):\n",
    "    ohdb_id = x.xxx.values[0]\n",
    "    x = x.drop(columns=['xxx'])\n",
    "    df_meteo = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_new.csv')\n",
    "    df_meteo['date'] = pd.to_datetime(df_meteo['date'])\n",
    "    df_meteo['ohdb_id'] = ohdb_id\n",
    "    x = x[['ohdb_id',name]].merge(df_meteo,on = 'ohdb_id')\n",
    "    x['tmp'] = (x.date - x[name]).dt.days\n",
    "    x3 = x.loc[(x.tmp>-3)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_3')\n",
    "    x7 = x.loc[(x.tmp>-7)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_7')\n",
    "    x15 = x.loc[(x.tmp>-15)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_15')\n",
    "    x30 = x.loc[(x.tmp>-30)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_30')\n",
    "    x365 = x.loc[(x.tmp>-365)&(x.tmp<=0),:].drop(columns=['date','ohdb_id','tmp']).groupby(name).mean().rename(columns=lambda x:x+'_365')\n",
    "    x = pd.concat([x3,x7,x15,x30,x365], axis = 1).reset_index()\n",
    "    return x\n",
    "\n",
    "df_flood = pd.read_csv('../data/dis_OHDB_seasonal4_Qmin7_Qmax7_1982-2023.csv')\n",
    "df_flood['Qmax7date'] = pd.to_datetime(df_flood['Qmax7date'])\n",
    "df_flood['Qmin7date'] = pd.to_datetime(df_flood['Qmin7date'])\n",
    "df_flood['Qmaxdate'] = pd.to_datetime(df_flood['Qmaxdate'])\n",
    "\n",
    "df_flood['xxx'] = df_flood['ohdb_id'].values\n",
    "\n",
    "df2 = df_flood.groupby('ohdb_id').p_apply(lambda x: func_meteo(x, name = 'Qmax7date')).reset_index().drop(columns = ['level_1'])\n",
    "df2.to_csv('../data/Qmax7_seasonal4_multi_MSWX_meteo.csv', index = False)\n",
    "del df2\n",
    "\n",
    "df2 = df_flood.groupby('ohdb_id').p_apply(lambda x: func_meteo(x, name = 'Qmin7date')).reset_index().drop(columns = ['level_1'])\n",
    "df2.to_csv('../data/Qmin7_seasonal4_multi_MSWX_meteo.csv', index = False)\n",
    "del df2\n",
    "\n",
    "df2 = df_flood.groupby('ohdb_id').p_apply(lambda x: func_meteo(x, name = 'Qmaxdate')).reset_index().drop(columns = ['level_1'])\n",
    "df2.to_csv('../data/Qmax_seasonal4_multi_MSWX_meteo.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910bed7",
   "metadata": {},
   "source": [
    "### subset basin boundary files again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec385a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dis_OHDB_Qmin7_Qmax7_1982-2023.csv')\n",
    "ohdb_ids = df.ohdb_id.unique()\n",
    "for fname in glob.glob('../basin_boundary/GRIT*8857*'):\n",
    "    gdf = read_dataframe(fname)\n",
    "    gdf = gdf.loc[gdf.ohdb_id.isin(ohdb_ids),:]\n",
    "    write_dataframe(gdf, fname[:-5]+'_subset.gpkg')\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b20a1",
   "metadata": {},
   "source": [
    "### use catch_mean_GLHYMPS_GLiM.py to get catchment average subsurface characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff8dce",
   "metadata": {},
   "source": [
    "### calculate number of upstream dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_dam = read_dataframe('../../data/geography/GDAT_data_v1/data/GDAT_v1_dams.shp')\n",
    "gdf_dam = gdf_dam.to_crs('espg:8857')\n",
    "gdf = read_dataframe('../basin_boundary/GRIT_full_catchment_all_EPSG8857_simplify_final_125km2_subset.gpkg')\n",
    "join = gpd.overlay(gdf_dam, gdf)\n",
    "join = join.groupby('ohdb_id')['Feature_ID'].count().rename(columns={'Feature_ID':'dam_num'})\n",
    "join = join.reindex(gdf.ohdb_id.values).fillna(0).reset_index()\n",
    "join.to_csv('../geography/dam_num.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c7652",
   "metadata": {},
   "source": [
    "### extract average meteorological conditions in the past 7 days preceding Qmax7 and Qmin7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617a3d6",
   "metadata": {},
   "source": [
    "### merge basin attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84ffa8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10717, 60)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob,os\n",
    "\n",
    "fnames = glob.glob('../geography/*csv')\n",
    "df_all = []\n",
    "for fname in fnames:\n",
    "    df = pd.read_csv(fname)\n",
    "    if df.shape[1] >= 10717:\n",
    "        df = df.T.reset_index()\n",
    "    if df.shape[0] >= 10717:\n",
    "        df = df\n",
    "    name = os.path.basename(fname).split('_')[0]\n",
    "    try:\n",
    "        df = df.loc[df['index']!='time',:]\n",
    "    except:\n",
    "        df = df.loc[df.ohdb_id!='time',:]\n",
    "    df = df.rename(columns={'index':'ohdb_id'}).set_index('ohdb_id')\n",
    "    if '0-5cm' in fname:\n",
    "        name = name + '_layer1'\n",
    "    elif '5-15cm' in fname:\n",
    "        name = name + '_layer2'\n",
    "    elif '15-30cm' in fname:\n",
    "        name = name + '_layer3'\n",
    "    elif '30-60cm' in fname:\n",
    "        name = name + '_layer4'\n",
    "    elif '60-100cm' in fname:\n",
    "        name = name + '_layer5'\n",
    "    elif '100-200cm' in fname:\n",
    "        name = name + '_layer6'\n",
    "    if df.shape[1] == 1:\n",
    "        df.columns = [name]\n",
    "    if 'LAI' in fname:\n",
    "        df = df.iloc[:-1,:2]\n",
    "        df.columns = ['LAI','FAPAR']\n",
    "    df_all.append(df)\n",
    "df_all = pd.concat(df_all, axis = 1)\n",
    "df_all = df_all.reset_index().rename(columns={'index':'ohdb_id'})\n",
    "\n",
    "# merge metadata\n",
    "df_meta = pd.read_csv('../data/OHDB_metadata_subset.csv')\n",
    "df_all = df_all.merge(df_meta, on = 'ohdb_id')\n",
    "\n",
    "df_all.to_csv('../data/basin_attributes_new.csv', index = False)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78212bd",
   "metadata": {},
   "source": [
    "### use GDAT reservoir area / catchment area to indicate the impact of reservoir regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb162d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_res = read_dataframe('../../data/geography/GDAT_data_v1/data/GDAT_v1_catchments.shp').to_crs('epsg:8857')\n",
    "gdf_dam = read_dataframe('../../data/geography/GDAT_data_v1/data/GDAT_v1_dams.shp').to_crs('epsg:8857')\n",
    "gdf_basin = read_dataframe('../basin_boundary/GRIT_full_catchment_all_EPSG8857_simplify_final_125km2_subset.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52193d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = gpd.sjoin(gdf_basin, gdf_dam)\n",
    "gdf_res['darea'] = gdf_res.area / 1000000\n",
    "inter = inter.merge(gdf_res[['Feature_ID','Dam_Name','darea']], on = ['Feature_ID','Dam_Name'])\n",
    "inter.loc[inter.Year_Fin=='BLANK','Year_Fin'] = None\n",
    "inter['Year_Fin'] = pd.to_numeric(inter['Year_Fin'])\n",
    "\n",
    "inter.loc[inter.Year_Const.isna(),'Year_Const'] = np.nan\n",
    "inter.loc[~inter.Year_Const.isna(),'Year_Const'] = inter.loc[~inter.Year_Const.isna(),'Year_Const'].str[:4].astype(int)\n",
    "inter['year'] = inter[['Year_Fin','Year_Const']].min(axis=1)\n",
    "inter = inter.groupby(['ohdb_id','gritDarea']).apply(lambda x:pd.Series([\n",
    "    x.darea.sum(),\n",
    "    x.year.mean(),\n",
    "    x.Main_P_Map.mode().values[0] if len(x.Main_P_Map.mode().values)>=1 else 'Hydroelectricity'\n",
    "], index = ['res_darea_normalize','Year_ave','Main_Purpose_mode'])).reset_index()\n",
    "inter['Year_ave'] = inter['Year_ave'].fillna(2000)\n",
    "inter['Main_Purpose_mode'] = inter['Main_Purpose_mode'].fillna('Hydroelectricity')\n",
    "inter['res_darea_normalize'] = inter.res_darea_normalize / inter.gritDarea\n",
    "inter.to_csv('../data/dam_impact.csv', index = False)\n",
    "print(inter.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e37048",
   "metadata": {},
   "source": [
    "### calculate annual average and std of Tmax and Tmin as basin attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "fnames = glob.glob('../data_mswx/*meteo*')\n",
    "def func(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df.set_index('ohdb_id')\n",
    "    df = df.loc[:,(df.columns.str.contains('Tmax'))|(df.columns.str.contains('Tmin'))]\n",
    "    return (df)\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(8)\n",
    "df = pool.map(func, fnames)\n",
    "df = pd.concat(df, axis = 1)\n",
    "df_Tmax = pd.concat([\n",
    "    df.loc[:,df.columns.str.contains('Tmax')].mean(axis = 1),\n",
    "    df.loc[:,df.columns.str.contains('Tmax')].std(axis = 1)\n",
    "], axis = 1)\n",
    "df_Tmin = pd.concat([\n",
    "    df.loc[:,df.columns.str.contains('Tmin')].mean(axis = 1),\n",
    "    df.loc[:,df.columns.str.contains('Tmin')].std(axis = 1)\n",
    "], axis = 1)\n",
    "\n",
    "df_Tmax.columns = ['tmax_ave','tmax_std']\n",
    "df_Tmin.columns = ['tmin_ave','tmin_std']\n",
    "df_Tmax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b122f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tmax.columns = ['tmax_ave','tmax_std']\n",
    "df_Tmin.columns = ['tmin_ave','tmin_std']\n",
    "df_attr = pd.read_csv('../data/basin_attributes.csv').set_index('ohdb_id')\n",
    "df_attr = df_attr.loc[:,~df_attr.columns.str.contains('tmax')]\n",
    "df_attr = df_attr.loc[:,~df_attr.columns.str.contains('tmin')]\n",
    "df_attr = pd.concat([df_Tmax, df_attr, df_Tmin], axis = 1).reset_index()\n",
    "df_attr.to_csv('../data/basin_attributes.csv', index = False)\n",
    "\n",
    "df_attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa4ce534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ohdb_id</th>\n",
       "      <th>Qmax7date</th>\n",
       "      <th>p_3</th>\n",
       "      <th>tmax_3</th>\n",
       "      <th>tmin_3</th>\n",
       "      <th>lwd_3</th>\n",
       "      <th>pres_3</th>\n",
       "      <th>relhum_3</th>\n",
       "      <th>spechum_3</th>\n",
       "      <th>swd_3</th>\n",
       "      <th>...</th>\n",
       "      <th>relhum_365</th>\n",
       "      <th>spechum_365</th>\n",
       "      <th>swd_365</th>\n",
       "      <th>wind_365</th>\n",
       "      <th>snowfall_365</th>\n",
       "      <th>snowmelt_365</th>\n",
       "      <th>snow_365</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OHDB_001000001</td>\n",
       "      <td>1982-02-23</td>\n",
       "      <td>12.130000</td>\n",
       "      <td>29.746667</td>\n",
       "      <td>22.396667</td>\n",
       "      <td>416.689829</td>\n",
       "      <td>97624.539062</td>\n",
       "      <td>89.719617</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>182.427806</td>\n",
       "      <td>...</td>\n",
       "      <td>88.309941</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>193.707798</td>\n",
       "      <td>1.033889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DJF</td>\n",
       "      <td>1982</td>\n",
       "      <td>141.059424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OHDB_001000001</td>\n",
       "      <td>1982-04-14</td>\n",
       "      <td>9.686667</td>\n",
       "      <td>29.323333</td>\n",
       "      <td>22.413333</td>\n",
       "      <td>412.342183</td>\n",
       "      <td>97680.229167</td>\n",
       "      <td>89.839584</td>\n",
       "      <td>0.018798</td>\n",
       "      <td>175.331294</td>\n",
       "      <td>...</td>\n",
       "      <td>88.365117</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>194.285508</td>\n",
       "      <td>0.998365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MAM</td>\n",
       "      <td>1982</td>\n",
       "      <td>94.455134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OHDB_001000001</td>\n",
       "      <td>1982-06-01</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>30.146667</td>\n",
       "      <td>19.896667</td>\n",
       "      <td>390.269114</td>\n",
       "      <td>98216.557292</td>\n",
       "      <td>82.823423</td>\n",
       "      <td>0.016004</td>\n",
       "      <td>197.838913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.407879</td>\n",
       "      <td>0.018295</td>\n",
       "      <td>189.422901</td>\n",
       "      <td>0.999079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>JJA</td>\n",
       "      <td>1982</td>\n",
       "      <td>16.824143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OHDB_001000001</td>\n",
       "      <td>1982-11-28</td>\n",
       "      <td>10.410000</td>\n",
       "      <td>30.016667</td>\n",
       "      <td>22.560000</td>\n",
       "      <td>414.350128</td>\n",
       "      <td>97579.018229</td>\n",
       "      <td>86.173724</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>166.310639</td>\n",
       "      <td>...</td>\n",
       "      <td>88.696003</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>194.952964</td>\n",
       "      <td>0.921536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SON</td>\n",
       "      <td>1982</td>\n",
       "      <td>96.043722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OHDB_001000001</td>\n",
       "      <td>1983-02-25</td>\n",
       "      <td>8.410000</td>\n",
       "      <td>29.716667</td>\n",
       "      <td>23.350000</td>\n",
       "      <td>424.421173</td>\n",
       "      <td>97658.708333</td>\n",
       "      <td>89.706668</td>\n",
       "      <td>0.019485</td>\n",
       "      <td>193.830856</td>\n",
       "      <td>...</td>\n",
       "      <td>88.290538</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>198.201429</td>\n",
       "      <td>0.936685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DJF</td>\n",
       "      <td>1983</td>\n",
       "      <td>76.552865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ohdb_id  Qmax7date        p_3     tmax_3     tmin_3       lwd_3  \\\n",
       "0  OHDB_001000001 1982-02-23  12.130000  29.746667  22.396667  416.689829   \n",
       "1  OHDB_001000001 1982-04-14   9.686667  29.323333  22.413333  412.342183   \n",
       "2  OHDB_001000001 1982-06-01   0.316667  30.146667  19.896667  390.269114   \n",
       "3  OHDB_001000001 1982-11-28  10.410000  30.016667  22.560000  414.350128   \n",
       "4  OHDB_001000001 1983-02-25   8.410000  29.716667  23.350000  424.421173   \n",
       "\n",
       "         pres_3   relhum_3  spechum_3       swd_3  ...  relhum_365  \\\n",
       "0  97624.539062  89.719617   0.018836  182.427806  ...   88.309941   \n",
       "1  97680.229167  89.839584   0.018798  175.331294  ...   88.365117   \n",
       "2  98216.557292  82.823423   0.016004  197.838913  ...   88.407879   \n",
       "3  97579.018229  86.173724   0.018418  166.310639  ...   88.696003   \n",
       "4  97658.708333  89.706668   0.019485  193.830856  ...   88.290538   \n",
       "\n",
       "   spechum_365     swd_365  wind_365  snowfall_365  snowmelt_365  snow_365  \\\n",
       "0     0.018450  193.707798  1.033889           0.0           0.0       0.0   \n",
       "1     0.018500  194.285508  0.998365           0.0           0.0       0.0   \n",
       "2     0.018295  189.422901  0.999079           0.0           0.0       0.0   \n",
       "3     0.018305  194.952964  0.921536           0.0           0.0       0.0   \n",
       "4     0.018455  198.201429  0.936685           0.0           0.0       0.0   \n",
       "\n",
       "   season  year           Q  \n",
       "0     DJF  1982  141.059424  \n",
       "1     MAM  1982   94.455134  \n",
       "2     JJA  1982   16.824143  \n",
       "3     SON  1982   96.043722  \n",
       "4     DJF  1983   76.552865  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "target = 'Qmax7'\n",
    "# read meteo dataset\n",
    "fname = f'../data/{target}_seasonal4_multi_MSWX_meteo.csv'\n",
    "df_meteo = pd.read_csv(fname)\n",
    "\n",
    "# merge with discharge\n",
    "df_dis = pd.read_csv('../data/dis_OHDB_seasonal4_Qmin7_Qmax7_1982-2023.csv')\n",
    "\n",
    "df_meteo[target+'date'] = pd.to_datetime(df_meteo[target+'date'])\n",
    "df_dis[target+'date'] = pd.to_datetime(df_dis[target+'date'])\n",
    "\n",
    "df_meteo = df_meteo.merge(df_dis[[target+'date','season','year',target, 'ohdb_id']], on = ['ohdb_id', target+'date'])\n",
    "df_meteo = df_meteo.rename(columns={target:'Q'})\n",
    "\n",
    "df_meteo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06b5a6",
   "metadata": {},
   "source": [
    "### connect gauge to Koppen 2nd climate classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98682f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHDB_007009994\n",
      "OHDB_014000155\n",
      "OHDB_012001727\n",
      "OHDB_015000013\n",
      "OHDB_007009344\n",
      "OHDB_008006375\n",
      "OHDB_007010152\n",
      "OHDB_008001457\n",
      "OHDB_008001517\n",
      "OHDB_008006308\n",
      "OHDB_008006399\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_table('../../data/geography/koppen_map_1980-2099/legend.txt', skiprows = 3, nrows = 30, header = None)\n",
    "df.columns = ['tmp']\n",
    "df['koppen_code'] = df.tmp.apply(lambda x:int(x.split(':')[0]))\n",
    "df['koppen_label'] = df.tmp.apply(lambda x:x.split(':')[1].split()[0])\n",
    "\n",
    "df['koppen_text1'] = df.tmp.apply(lambda x:x.split(':')[1].split(',')[0].split()[1])\n",
    "df['koppen_text2'] = df.tmp.apply(lambda x:x.split(':')[1].split(',')[1])\n",
    "df['koppen_text2'] = df['koppen_text2'].apply(lambda x:x.split()[0].strip() if '[' in x else x)\n",
    "df['koppen_text3'] = df.tmp.apply(lambda x:x.split(':')[1].split(',')[-1])\n",
    "df['koppen_text3'] = df['koppen_text3'].str.extract(r\"([^\\d]+)\")\n",
    "df['koppen_text3'] = df['koppen_text3'].apply(lambda x:x[:-1].strip())\n",
    "df['koppen_text3'] = np.where(df['koppen_text3'] == df['koppen_text2'], None, df['koppen_text3'])\n",
    "df = df.drop(columns=['tmp'])\n",
    "\n",
    "import xarray as xr\n",
    "ds = xr.open_dataset('../../data/geography/koppen_map_1980-2099/1991_2020/koppen_geiger_0p01.nc')\n",
    "df1 = pd.read_csv('../data/basin_attributes.csv')\n",
    "koppen = []\n",
    "for ohdb_id0,lon0,lat0 in zip(df1.ohdb_id.values,df1.ohdb_longitude.values, df1.ohdb_latitude.values):\n",
    "    val0 = ds.sel(lon = lon0, lat = lat0, method = 'nearest').koppen.values\n",
    "    if np.isnan(val0):\n",
    "        print(ohdb_id0)\n",
    "    df0 = df.loc[df.koppen_code==val0,:].reset_index(drop=True)\n",
    "    df0['ohdb_id'] = ohdb_id0\n",
    "    koppen.append(df0)\n",
    "koppen = pd.concat(koppen)\n",
    "df1 = df1.merge(koppen, on = 'ohdb_id')\n",
    "df1.to_csv('../data/basin_attributes_koppen.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d4fde64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Tmax</th>\n",
       "      <th>Tmin</th>\n",
       "      <th>SWd</th>\n",
       "      <th>Pres</th>\n",
       "      <th>RelHum</th>\n",
       "      <th>SpecHum</th>\n",
       "      <th>LWd</th>\n",
       "      <th>Wind</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>snow</th>\n",
       "      <th>snowmelt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohdb_id</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">OHDB_009000845</th>\n",
       "      <th>1981-01-01</th>\n",
       "      <td>4.960733</td>\n",
       "      <td>21.503464</td>\n",
       "      <td>14.394141</td>\n",
       "      <td>197.006973</td>\n",
       "      <td>82042.835938</td>\n",
       "      <td>89.206520</td>\n",
       "      <td>0.013872</td>\n",
       "      <td>362.504364</td>\n",
       "      <td>1.267897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>0.996564</td>\n",
       "      <td>21.936342</td>\n",
       "      <td>13.685956</td>\n",
       "      <td>261.358429</td>\n",
       "      <td>82063.757812</td>\n",
       "      <td>86.056458</td>\n",
       "      <td>0.013364</td>\n",
       "      <td>351.786194</td>\n",
       "      <td>1.476641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-03</th>\n",
       "      <td>0.942824</td>\n",
       "      <td>23.212969</td>\n",
       "      <td>15.353713</td>\n",
       "      <td>257.343262</td>\n",
       "      <td>81964.789062</td>\n",
       "      <td>83.134964</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>357.235016</td>\n",
       "      <td>1.403453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-04</th>\n",
       "      <td>1.474520</td>\n",
       "      <td>23.235859</td>\n",
       "      <td>14.299811</td>\n",
       "      <td>253.723099</td>\n",
       "      <td>81921.781250</td>\n",
       "      <td>80.910568</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>357.349884</td>\n",
       "      <td>1.580318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>2.216167</td>\n",
       "      <td>22.546692</td>\n",
       "      <td>14.769879</td>\n",
       "      <td>264.795990</td>\n",
       "      <td>81981.890625</td>\n",
       "      <td>81.769661</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>357.425964</td>\n",
       "      <td>1.587152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">OHDB_004000197</th>\n",
       "      <th>1981-12-27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.006573</td>\n",
       "      <td>23.601622</td>\n",
       "      <td>356.596588</td>\n",
       "      <td>99503.234375</td>\n",
       "      <td>62.263279</td>\n",
       "      <td>0.015858</td>\n",
       "      <td>391.136322</td>\n",
       "      <td>2.072009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-12-28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.264484</td>\n",
       "      <td>26.438501</td>\n",
       "      <td>333.311096</td>\n",
       "      <td>99575.695312</td>\n",
       "      <td>68.274765</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>410.273987</td>\n",
       "      <td>3.301222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-12-29</th>\n",
       "      <td>1.523697</td>\n",
       "      <td>34.614761</td>\n",
       "      <td>26.281151</td>\n",
       "      <td>305.810211</td>\n",
       "      <td>99908.812500</td>\n",
       "      <td>72.394539</td>\n",
       "      <td>0.019430</td>\n",
       "      <td>425.678711</td>\n",
       "      <td>3.667662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-12-30</th>\n",
       "      <td>0.097780</td>\n",
       "      <td>34.238354</td>\n",
       "      <td>24.772182</td>\n",
       "      <td>290.525543</td>\n",
       "      <td>100040.312500</td>\n",
       "      <td>68.747940</td>\n",
       "      <td>0.017634</td>\n",
       "      <td>414.837036</td>\n",
       "      <td>3.042900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-12-31</th>\n",
       "      <td>0.085665</td>\n",
       "      <td>34.765869</td>\n",
       "      <td>23.399099</td>\n",
       "      <td>320.861359</td>\n",
       "      <td>100057.140625</td>\n",
       "      <td>67.658249</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>401.011902</td>\n",
       "      <td>2.774102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3911705 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  P       Tmax       Tmin         SWd  \\\n",
       "ohdb_id        time                                                     \n",
       "OHDB_009000845 1981-01-01  4.960733  21.503464  14.394141  197.006973   \n",
       "               1981-01-02  0.996564  21.936342  13.685956  261.358429   \n",
       "               1981-01-03  0.942824  23.212969  15.353713  257.343262   \n",
       "               1981-01-04  1.474520  23.235859  14.299811  253.723099   \n",
       "               1981-01-05  2.216167  22.546692  14.769879  264.795990   \n",
       "...                             ...        ...        ...         ...   \n",
       "OHDB_004000197 1981-12-27  0.000000  37.006573  23.601622  356.596588   \n",
       "               1981-12-28  0.000000  37.264484  26.438501  333.311096   \n",
       "               1981-12-29  1.523697  34.614761  26.281151  305.810211   \n",
       "               1981-12-30  0.097780  34.238354  24.772182  290.525543   \n",
       "               1981-12-31  0.085665  34.765869  23.399099  320.861359   \n",
       "\n",
       "                                    Pres     RelHum   SpecHum         LWd  \\\n",
       "ohdb_id        time                                                         \n",
       "OHDB_009000845 1981-01-01   82042.835938  89.206520  0.013872  362.504364   \n",
       "               1981-01-02   82063.757812  86.056458  0.013364  351.786194   \n",
       "               1981-01-03   81964.789062  83.134964  0.013904  357.235016   \n",
       "               1981-01-04   81921.781250  80.910568  0.013259  357.349884   \n",
       "               1981-01-05   81981.890625  81.769661  0.013302  357.425964   \n",
       "...                                  ...        ...       ...         ...   \n",
       "OHDB_004000197 1981-12-27   99503.234375  62.263279  0.015858  391.136322   \n",
       "               1981-12-28   99575.695312  68.274765  0.018731  410.273987   \n",
       "               1981-12-29   99908.812500  72.394539  0.019430  425.678711   \n",
       "               1981-12-30  100040.312500  68.747940  0.017634  414.837036   \n",
       "               1981-12-31  100057.140625  67.658249  0.016210  401.011902   \n",
       "\n",
       "                               Wind  snowfall  snow  snowmelt  \n",
       "ohdb_id        time                                            \n",
       "OHDB_009000845 1981-01-01  1.267897       0.0   0.0       0.0  \n",
       "               1981-01-02  1.476641       0.0   0.0       0.0  \n",
       "               1981-01-03  1.403453       0.0   0.0       0.0  \n",
       "               1981-01-04  1.580318       0.0   0.0       0.0  \n",
       "               1981-01-05  1.587152       0.0   0.0       0.0  \n",
       "...                             ...       ...   ...       ...  \n",
       "OHDB_004000197 1981-12-27  2.072009       0.0   0.0       0.0  \n",
       "               1981-12-28  3.301222       0.0   0.0       0.0  \n",
       "               1981-12-29  3.667662       0.0   0.0       0.0  \n",
       "               1981-12-30  3.042900       0.0   0.0       0.0  \n",
       "               1981-12-31  2.774102       0.0   0.0       0.0  \n",
       "\n",
       "[3911705 rows x 12 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames1 = glob.glob('../data_mswx/GRIT_catch_ave_std*1981*')\n",
    "fnames2 = glob.glob('../data_mswx/*snow*1981*csv')\n",
    "df1 = []\n",
    "for fname in fnames1:\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df.loc[df.stat=='ave',:].drop(columns=['stat'])\n",
    "    df = df.melt(id_vars = 'time')\n",
    "    df = df.rename(columns={'variable':'ohdb_id','value':fname.split('_')[-2]})\n",
    "    df = df.set_index(['ohdb_id','time'])\n",
    "    df1.append(df)\n",
    "df1 = pd.concat(df1, axis = 1)\n",
    "df2 = []\n",
    "for fname in fnames2:\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df.melt(id_vars = 'time').rename(columns={'variable':'ohdb_id','value':fname.split('_')[-2]})\n",
    "    df = df.set_index(['ohdb_id','time'])\n",
    "    df2.append(df)\n",
    "df2 = pd.concat(df2, axis = 1)\n",
    "df = pd.concat([df1, df2], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e81ce2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46202291ac5e49998829a7aac6b1948c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<LAMBDA> DONE:   0%|          | 0/10717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index().rename(columns={'time':'date'})\n",
    "df.groupby('ohdb_id').p_apply(lambda x:x.drop(columns=['ohdb_id']).to_csv(f'../data_mswx/mswx_each_basin/meteo_{x.ohdb_id.values[0]}_1981.csv',index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e389627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(ohdb_id):\n",
    "    df = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}.csv')\n",
    "    df = df.drop(columns=['p_mean','p_std'])\n",
    "    df1 = pd.read_csv(f'../data_mswx/mswx_each_basin/meteo_{ohdb_id}_1981.csv')\n",
    "    df1.columns = [a.lower() for a in df1.columns]\n",
    "    out = pd.concat([df, df1])\n",
    "    df2 = pd.read_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_mswep_gleam.csv')\n",
    "    out = out.merge(df2, on = 'date')\n",
    "    out.to_csv(f'../data_mswx/mswx_each_basin/{ohdb_id}_1981-2023.csv', index = False)\n",
    "ohdb_ids = pd.read_csv('../data/basin_attributes.csv')\n",
    "ohdb_ids = ohdb_ids.ohdb_id.values\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(12)\n",
    "s = pool.map(func, ohdb_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12eaf27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10586,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/dis_OHDB_seasonal4_Qmin7_Qmax7_1982-2023.csv')\n",
    "df.ohdb_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python plot_basin_attr.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ee134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
